{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.1.88:4040\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = local[*], app id = local-1572451134780)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world ! from Trainer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.SparkConf\n",
       "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.SparkConf\n",
       "import org.apache.spark.ml.feature.{RegexTokenizer, StopWordsRemover, StringIndexer, CountVectorizer, CountVectorizerModel, VectorAssembler, IDF, OneHotEncoderEstimator}\n",
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "conf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@20ba0352\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@4c1ca7a\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.ml.feature.{RegexTokenizer,StopWordsRemover,\n",
    "                                    StringIndexer,CountVectorizer,\n",
    "                                    CountVectorizerModel,VectorAssembler,\n",
    "                                   IDF,OneHotEncoderEstimator}\n",
    "\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n",
    "\n",
    "// import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
    "// import org.apache.spark.ml.evaluation\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "\n",
    "\n",
    "val conf = new SparkConf().setAll(Map(\n",
    "      \"spark.scheduler.mode\" -> \"FIFO\",\n",
    "      \"spark.speculation\" -> \"false\",\n",
    "      \"spark.reducer.maxSizeInFlight\" -> \"48m\",\n",
    "      \"spark.serializer\" -> \"org.apache.spark.serializer.KryoSerializer\",\n",
    "      \"spark.kryoserializer.buffer.max\" -> \"1g\",\n",
    "      \"spark.shuffle.file.buffer\" -> \"32k\",\n",
    "      \"spark.default.parallelism\" -> \"12\",\n",
    "      \"spark.sql.shuffle.partitions\" -> \"12\",\n",
    "      \"spark.driver.maxResultSize\" -> \"2g\"\n",
    "    ))\n",
    "\n",
    "val spark = SparkSession\n",
    "  .builder\n",
    "  .config(conf)\n",
    "  .appName(\"TP Spark : Trainer\")\n",
    "  .getOrCreate()\n",
    "\n",
    "\n",
    "/*******************************************************************************\n",
    "  *\n",
    "  *       TP 3\n",
    "  *\n",
    "  *       - lire le fichier sauvegarder précédemment\n",
    "  *       - construire les Stages du pipeline, puis les assembler\n",
    "  *       - trouver les meilleurs hyperparamètres pour l'entraînement du pipeline avec une grid-search\n",
    "  *       - Sauvegarder le pipeline entraîné\n",
    "  *\n",
    "  *       if problems with unimported modules => sbt plugins update\n",
    "  *\n",
    "  ********************************************************************************/\n",
    "\n",
    "println(\"hello world ! from Trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 12 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df:DataFrame = spark.read.parquet(\"/home/jorge/Documents/Cours/Spark/RepoAdotTPs/data/prepared_trainingset/\")\n",
    "\n",
    "// df.select(\"project_id\", \"name\", \"desc\", \"goal\").show(5)\n",
    "// df.select(\"keywords\", \"final_status\", \"country2\", \"currency2\").show(5)\n",
    "// df.select(\"deadline2\", \"created_at2\", \"launched_at2\", \"days_campaign\").show(5)\n",
    "// df.select(\"hours_prepa\", \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer: org.apache.spark.ml.feature.RegexTokenizer = regexTok_aa96344723f5\n",
       "remover: org.apache.spark.ml.feature.StopWordsRemover = stopWords_9cc38643efbf\n",
       "cvModel: org.apache.spark.ml.feature.CountVectorizer = cntVec_fd09521beb46\n",
       "idf: org.apache.spark.ml.feature.IDF = idf_41061e223cb2\n",
       "indexerCountry: org.apache.spark.ml.feature.StringIndexer = strIdx_3483fbbf35e5\n",
       "indexerCurrency: org.apache.spark.ml.feature.StringIndexer = strIdx_6ab476750a62\n",
       "encoder: org.apache.spark.ml.feature.OneHotEncoderEstimator = oneHotEncoder_f0ff38978403\n",
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_6420eefb8ffc\n",
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_7240eeb9b1b1\n"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tokenizer = new RegexTokenizer()\n",
    "  .setPattern(\"\\\\W+\")\n",
    "  .setGaps(true)\n",
    "  .setInputCol(\"text\")\n",
    "  .setOutputCol(\"tokens\")\n",
    "// val dfTokenized = tokenizer.transform(df)\n",
    "\n",
    "\n",
    "val remover = new StopWordsRemover()\n",
    "  .setInputCol(tokenizer.getOutputCol)\n",
    "  .setOutputCol(\"filtered\")\n",
    "\n",
    "// val dfsw = remover.transform(dfTokenized)\n",
    " \n",
    "\n",
    "val cvModel: CountVectorizer = new CountVectorizer()\n",
    "    .setInputCol(remover.getOutputCol)\n",
    "    .setOutputCol(\"vect\")\n",
    "    .setMinDF(50)\n",
    "    \n",
    "    \n",
    "\n",
    "// val dfv = cvModel.fit(dfsw).transform(dfsw)\n",
    "\n",
    "\n",
    "\n",
    "val idf = new IDF()\n",
    "    .setInputCol(cvModel.getOutputCol)\n",
    "    .setOutputCol(\"tfidf\")\n",
    "\n",
    "// val idfModel = idf.fit(dfv)\n",
    "\n",
    "// val rescaledData = idfModel.transform(dfv)\n",
    "                                      \n",
    "\n",
    "val indexerCountry = new StringIndexer()\n",
    "  .setInputCol(\"country2\")\n",
    "  .setOutputCol(\"country_indexed\")\n",
    "\n",
    "val indexerCurrency = new StringIndexer()\n",
    "  .setInputCol(\"currency2\")\n",
    "  .setOutputCol(\"currency_indexed\")\n",
    "\n",
    "\n",
    "// val indexedCountry = indexerCountry.fit(rescaledData).transform(rescaledData)\n",
    "// val indexedCountryCurrency = indexerCurrency.fit(indexedCountry).transform(indexedCountry)\n",
    "\n",
    "\n",
    "val encoder = new OneHotEncoderEstimator()\n",
    "  .setInputCols(Array(\"country_indexed\", \"currency_indexed\"))\n",
    "  .setOutputCols(Array(\"country_onehot\", \"currency_onehot\"))\n",
    "\n",
    "// val model = encoder.fit(indexedCountryCurrency)\n",
    "// val encoded = model.transform(indexedCountryCurrency)\n",
    "\n",
    "\n",
    "val assembler = new VectorAssembler()\n",
    "  .setInputCols(Array(\"tfidf\",\"days_campaign\",\"hours_prepa\",\"goal\",\"country_onehot\",\"currency_onehot\"))\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "val lr = new LogisticRegression()\n",
    "  .setElasticNetParam(0.0)\n",
    "  .setFitIntercept(true)\n",
    "  .setFeaturesCol(\"features\")\n",
    "  .setLabelCol(\"final_status\")\n",
    "  .setStandardization(true)\n",
    "  .setPredictionCol(\"predictions\")\n",
    "  .setRawPredictionCol(\"raw_predictions\")\n",
    "  .setThresholds(Array(0.7, 0.3))\n",
    "  .setTol(1.0e-6)\n",
    "  .setMaxIter(20)\n",
    "\n",
    "// val transformed = assembler\n",
    "//     .setHandleInvalid(\"skip\")\n",
    "//     .transform(encoded)\n",
    "//     .drop(\"project_id\",\"name\",\"desc\",\"goal\",\"keywords\",\n",
    "//          \"country2\",\"currency2\",\"deadline2\",\"created_at2\",\"launched_at2\",\n",
    "//          \"days_campaign\",\"hours_prepa\",\"text\",\"tokens\",\"filtered\",\"vect\",\n",
    "//          \"country_indexed\",\"currency_indexed\",\"tfidf\",\"days_campaign\",\n",
    "//           \"hours_prepa\",\"goal\",\"country_onehot\",\"currency_onehot\")\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_a690d506a23b\n",
       "model: org.apache.spark.ml.PipelineModel = pipeline_a690d506a23b\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// transformed.show(numRows=5,truncate=false)\n",
    "\n",
    "val pipeline = new Pipeline()\n",
    "  .setStages(Array(tokenizer, remover,cvModel,idf,indexerCountry,\n",
    "                  indexerCurrency,encoder, assembler,lr ))\n",
    "\n",
    "// Fit the pipeline to training documents.\n",
    "val model = pipeline.fit(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+------------+\n",
      "|                text|predictions|final_status|\n",
      "+--------------------+-----------+------------+\n",
      "|american options ...|        1.0|           0|\n",
      "|iheadbones bone c...|        0.0|           0|\n",
      "|the fridge magazi...|        0.0|           0|\n",
      "|support new men's...|        0.0|           0|\n",
      "|can('t) a psychol...|        0.0|           0|\n",
      "|fragmented fate e...|        0.0|           0|\n",
      "|transport (suspen...|        0.0|           0|\n",
      "|the secret life o...|        0.0|           0|\n",
      "|cc survival decep...|        0.0|           0|\n",
      "|the best protein ...|        0.0|           0|\n",
      "|paradise falls pa...|        1.0|           0|\n",
      "|the chalet woodsh...|        1.0|           1|\n",
      "|vagabond mobile g...|        0.0|           0|\n",
      "|southern shakespe...|        1.0|           1|\n",
      "|leviathan: montau...|        1.0|           1|\n",
      "|the candle tray h...|        0.0|           0|\n",
      "|sun skin the miss...|        0.0|           0|\n",
      "|7sonic debut stud...|        0.0|           0|\n",
      "|the hades pit: a ...|        0.0|           0|\n",
      "|the fitness refin...|        0.0|           0|\n",
      "|the stone garden ...|        0.0|           0|\n",
      "|\"make da chedda m...|        0.0|           0|\n",
      "|highflight magazi...|        0.0|           0|\n",
      "|flour child creat...|        0.0|           0|\n",
      "|coming to you liv...|        0.0|           0|\n",
      "|the tiny tyrant's...|        0.0|           0|\n",
      "|tdi bassline the ...|        0.0|           0|\n",
      "|adventures of boo...|        0.0|           0|\n",
      "|rebeccas piano ta...|        0.0|           0|\n",
      "|patent art poster...|        1.0|           1|\n",
      "|a pigeon of bosto...|        1.0|           1|\n",
      "|'time at the bar!...|        0.0|           0|\n",
      "|bees and honey, h...|        0.0|           0|\n",
      "|quickscan discoun...|        0.0|           0|\n",
      "|help fund mags' j...|        0.0|           0|\n",
      "|self-publish my n...|        0.0|           0|\n",
      "|2015 mwg antholog...|        0.0|           0|\n",
      "|112cam. film a cr...|        0.0|           0|\n",
      "|black swamp - nat...|        1.0|           1|\n",
      "|help a lousir mak...|        0.0|           0|\n",
      "|craft-oriented co...|        0.0|           0|\n",
      "|\"support trez's d...|        0.0|           0|\n",
      "|spiritwalker born...|        0.0|           0|\n",
      "|farmer dreams [a ...|        0.0|           0|\n",
      "|music review webs...|        0.0|           0|\n",
      "|a walk through th...|        0.0|           0|\n",
      "|publishing a hist...|        0.0|           0|\n",
      "|john and clive th...|        0.0|           0|\n",
      "|holiday collectio...|        0.0|           0|\n",
      "|robotics competit...|        0.0|           0|\n",
      "|anise fine arts &...|        0.0|           0|\n",
      "|\"cut dui death wi...|        0.0|           0|\n",
      "|ithaca diaries, c...|        1.0|           1|\n",
      "|save the agawam c...|        1.0|           1|\n",
      "|ultimus qi: wirel...|        0.0|           0|\n",
      "|parts alley parts...|        0.0|           0|\n",
      "|lunicycle: the un...|        1.0|           1|\n",
      "|mural printing,di...|        0.0|           0|\n",
      "|the happy chef th...|        0.0|           0|\n",
      "|e marie fall15 ny...|        0.0|           0|\n",
      "|superstition moun...|        0.0|           0|\n",
      "|a function of rat...|        1.0|           1|\n",
      "|life interrupted ...|        0.0|           0|\n",
      "|relentless love e...|        1.0|           1|\n",
      "|'signing in' - ep...|        0.0|           0|\n",
      "|tomato juice toma...|        0.0|           0|\n",
      "|studio dance art ...|        0.0|           0|\n",
      "|the zen commuter ...|        0.0|           0|\n",
      "|thea: the awakeni...|        0.0|           0|\n",
      "|redneck candles &...|        0.0|           0|\n",
      "|2nd amendment wat...|        0.0|           0|\n",
      "|haunters - the mo...|        1.0|           1|\n",
      "|personally predic...|        0.0|           0|\n",
      "|amoeblobs an addi...|        0.0|           0|\n",
      "|crimefighting toa...|        0.0|           0|\n",
      "|kendama video tut...|        1.0|           1|\n",
      "|diy kit: 2015 cal...|        1.0|           1|\n",
      "|go-anywhere blank...|        1.0|           1|\n",
      "|the wheatones rec...|        1.0|           1|\n",
      "|o human star volu...|        1.0|           1|\n",
      "|fopydo smartstand...|        0.0|           0|\n",
      "|mamma's boy - the...|        0.0|           0|\n",
      "|bigphone videopho...|        0.0|           0|\n",
      "|\"polish \"\"\"\"\"\"\"\"\"...|        0.0|           0|\n",
      "|catfish catcher c...|        0.0|           0|\n",
      "|new york business...|        0.0|           0|\n",
      "|rankmatch - intel...|        0.0|           0|\n",
      "|godfall: empire o...|        0.0|           0|\n",
      "|so you want to wo...|        1.0|           0|\n",
      "|\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"...|        0.0|           0|\n",
      "|construction pape...|        1.0|           0|\n",
      "|'72 sols' a 3d ja...|        0.0|           0|\n",
      "|cryptos in print ...|        1.0|           1|\n",
      "|chasing ireland #...|        0.0|           0|\n",
      "|b-rabbit tv comed...|        1.0|           1|\n",
      "|our town's first ...|        0.0|           0|\n",
      "|the organ broker ...|        1.0|           1|\n",
      "|masala organic ac...|        0.0|           0|\n",
      "|the giving garden...|        1.0|           0|\n",
      "|plasma frequency ...|        0.0|           0|\n",
      "+--------------------+-----------+------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(df).columns\n",
    "\n",
    "model.transform(df).select(\"text\",\"predictions\",\"final_status\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [project_id: string, name: string ... 12 more fields]\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [project_id: string, name: string ... 12 more fields]\n",
       "size: (Long, Long) = (96910,10704)\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(train,test) = df.randomSplit(Array[Double](0.9, 0.1))\n",
    "val size = (train.count,test.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model2: org.apache.spark.ml.PipelineModel = pipeline_a690d506a23b\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model2 = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+--------------------+\n",
      "|final_status|predictions|         probability|\n",
      "+------------+-----------+--------------------+\n",
      "|           0|        1.0|[0.32634362590507...|\n",
      "|           0|        0.0|[0.99999999999840...|\n",
      "|           0|        0.0|[0.99990862947358...|\n",
      "|           0|        0.0|[0.99999997832714...|\n",
      "|           1|        0.0|[0.99999829968507...|\n",
      "|           0|        0.0|[0.99999997618484...|\n",
      "|           0|        1.0|[6.07975854013273...|\n",
      "|           0|        1.0|[0.07003446318569...|\n",
      "|           1|        1.0|[1.14444280953689...|\n",
      "|           0|        0.0|[0.99999999862510...|\n",
      "|           1|        1.0|[1.88378357933403...|\n",
      "|           0|        1.0|[0.43119273106945...|\n",
      "|           0|        0.0|[0.99999999696719...|\n",
      "|           1|        0.0|[0.99999999911144...|\n",
      "|           0|        0.0|[0.99972014926533...|\n",
      "|           1|        1.0|[4.72478290707708...|\n",
      "|           0|        0.0|[0.99994963921231...|\n",
      "|           0|        0.0|[0.98645639926079...|\n",
      "|           0|        0.0|[0.99999999749527...|\n",
      "|           1|        1.0|[2.41134828669885...|\n",
      "|           0|        0.0|[0.99999998185763...|\n",
      "|           1|        1.0|[0.69174077052975...|\n",
      "|           0|        0.0|[0.99991777769928...|\n",
      "|           0|        0.0|[0.99999972841560...|\n",
      "|           0|        0.0|[0.99668212290837...|\n",
      "|           0|        0.0|[1.0,1.5194453025...|\n",
      "|           0|        0.0|[0.76311139289097...|\n",
      "|           0|        0.0|[0.93216545092292...|\n",
      "|           1|        0.0|[0.99999584007093...|\n",
      "|           0|        0.0|[0.99999523404807...|\n",
      "|           0|        0.0|[0.91602497494654...|\n",
      "|           0|        1.0|[1.44892903596393...|\n",
      "|           0|        0.0|[0.87488316167501...|\n",
      "|           0|        1.0|[0.05192054782134...|\n",
      "|           0|        0.0|[0.99642830528234...|\n",
      "|           0|        1.0|[0.67464015822382...|\n",
      "|           0|        0.0|[0.99968846875594...|\n",
      "|           0|        0.0|[0.99999999999931...|\n",
      "|           1|        0.0|[0.87795971009899...|\n",
      "|           0|        1.0|[0.25105278913542...|\n",
      "|           0|        0.0|[0.99944990695767...|\n",
      "|           0|        1.0|[6.48001332117877...|\n",
      "|           0|        1.0|[4.63845272118850...|\n",
      "|           1|        0.0|[0.99999999999818...|\n",
      "|           1|        0.0|[0.99999790760959...|\n",
      "|           1|        0.0|[0.96172408175319...|\n",
      "|           0|        0.0|[0.99810443655402...|\n",
      "|           0|        0.0|[0.99999215261720...|\n",
      "|           1|        0.0|[0.99981266898026...|\n",
      "|           0|        0.0|[0.97773590444648...|\n",
      "|           0|        0.0|[0.96488168702487...|\n",
      "|           1|        0.0|[0.87914940206511...|\n",
      "|           0|        0.0|[0.99981847231511...|\n",
      "|           1|        1.0|[0.63279410633018...|\n",
      "|           0|        1.0|[0.01500380269535...|\n",
      "|           1|        0.0|[0.99879972177850...|\n",
      "|           0|        0.0|[0.84558806631615...|\n",
      "|           0|        1.0|[0.03512744836536...|\n",
      "|           0|        1.0|[8.76374593477925...|\n",
      "|           0|        0.0|[0.99998987495740...|\n",
      "|           0|        0.0|[0.99999999998445...|\n",
      "|           0|        1.0|[0.00330502121361...|\n",
      "|           0|        0.0|[0.99999999988822...|\n",
      "|           1|        0.0|[0.99885658316829...|\n",
      "|           0|        1.0|[1.21268172494298...|\n",
      "|           1|        0.0|[0.99991311423842...|\n",
      "|           0|        0.0|[0.99998733153524...|\n",
      "|           0|        0.0|[0.99993697345864...|\n",
      "|           1|        0.0|[0.99999999856888...|\n",
      "|           0|        1.0|[0.67842632090060...|\n",
      "|           0|        1.0|[2.83499666381179...|\n",
      "|           0|        1.0|[4.46936649444111...|\n",
      "|           1|        0.0|[0.74375068402412...|\n",
      "|           0|        0.0|[1.0,4.3556842155...|\n",
      "|           0|        0.0|[0.99999941609213...|\n",
      "|           1|        1.0|[3.34164141608635...|\n",
      "|           0|        0.0|[0.99995409380852...|\n",
      "|           0|        1.0|[0.00803488301367...|\n",
      "|           0|        0.0|[0.99998923151429...|\n",
      "|           1|        0.0|[0.99999999999999...|\n",
      "|           0|        0.0|[0.99922903490804...|\n",
      "|           0|        1.0|[0.59565442698339...|\n",
      "|           0|        0.0|[0.99999999999994...|\n",
      "|           0|        0.0|[0.99999999999993...|\n",
      "|           0|        0.0|[0.99961057962329...|\n",
      "|           0|        0.0|[0.99986375901916...|\n",
      "|           0|        0.0|[0.99998805590349...|\n",
      "|           0|        0.0|[0.99998236834501...|\n",
      "|           0|        1.0|[2.34155364712733...|\n",
      "|           0|        1.0|[0.29521424771896...|\n",
      "|           0|        0.0|[0.92508829385594...|\n",
      "|           0|        0.0|[0.99999999985428...|\n",
      "|           0|        1.0|[0.68396761851703...|\n",
      "|           1|        1.0|[1.14645450347966...|\n",
      "|           0|        0.0|[0.99999999994504...|\n",
      "|           0|        0.0|[0.96920083424248...|\n",
      "|           1|        1.0|[0.07086570469840...|\n",
      "|           0|        0.0|[0.99829374456594...|\n",
      "|           0|        0.0|[0.99998377202205...|\n",
      "|           0|        1.0|[0.46167602213787...|\n",
      "+------------+-----------+--------------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 24 more fields]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions = model2.transform(test)\n",
    "\n",
    "predictions.select(\"final_status\",\"predictions\",\"probability\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "// // Clear the prediction threshold so the model will return probabilities\n",
    "// model2.clearThreshold\n",
    "\n",
    "// // Compute raw scores on the test set\n",
    "// val predictionAndLabels = test.map { case LabeledPoint(label, features) =>\n",
    "//   val prediction = model2.predict(features)\n",
    "//   (prediction, label)\n",
    "// }\n",
    "\n",
    "// // Instantiate metrics object\n",
    "// val metrics = new BinaryClassificationMetrics(predictionAndLabels)\n",
    "\n",
    "// // Precision by threshold\n",
    "// val precision = metrics.precisionByThreshold\n",
    "// precision.foreach { case (t, p) =>\n",
    "//   println(s\"Threshold: $t, Precision: $p\")\n",
    "// }\n",
    "\n",
    "// // Recall by threshold\n",
    "// val recall = metrics.recallByThreshold\n",
    "// recall.foreach { case (t, r) =>\n",
    "//   println(s\"Threshold: $t, Recall: $r\")\n",
    "// }\n",
    "\n",
    "// // Precision-Recall Curve\n",
    "// val PRC = metrics.pr\n",
    "\n",
    "// // F-measure\n",
    "// val f1Score = metrics.fMeasureByThreshold\n",
    "// f1Score.foreach { case (t, f) =>\n",
    "//   println(s\"Threshold: $t, F-score: $f, Beta = 1\")\n",
    "// }\n",
    "\n",
    "// val beta = 0.5\n",
    "// val fScore = metrics.fMeasureByThreshold(beta)\n",
    "// f1Score.foreach { case (t, f) =>\n",
    "//   println(s\"Threshold: $t, F-score: $f, Beta = 0.5\")\n",
    "// }\n",
    "\n",
    "// // AUPRC\n",
    "// val auPRC = metrics.areaUnderPR\n",
    "// println(s\"Area under precision-recall curve = $auPRC\")\n",
    "\n",
    "// // Compute thresholds used in ROC and PR curves\n",
    "// val thresholds = precision.map(_._1)\n",
    "\n",
    "// // ROC Curve\n",
    "// val roc = metrics.roc\n",
    "\n",
    "// // AUROC\n",
    "// val auROC = metrics.areaUnderROC\n",
    "// println(s\"Area under ROC = $auROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.6205254720418814\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_7a5033ea15df\n",
       "f1: Double = 0.6205254720418814\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "      .setLabelCol(\"final_status\")\n",
    "      .setPredictionCol(\"predictions\")\n",
    "      .setMetricName(\"f1\")\n",
    "\n",
    "val f1 = evaluator.evaluate(predictions)\n",
    "println(\"Test set accuracy = \" + f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder, CrossValidatorModel, TrainValidationSplit}\n",
       "import org.apache.spark.ml.param.ParamMap\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder,CrossValidatorModel,TrainValidationSplit}\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tlogreg_ba37d9389c1d-regParam: 1.0E-7\n",
       "}, {\n",
       "\tlogreg_ba37d9389c1d-regParam: 1.0E-5\n",
       "}, {\n",
       "\tlogreg_ba37d9389c1d-regParam: 0.001\n",
       "}, {\n",
       "\tlogreg_ba37d9389c1d-regParam: 0.1\n",
       "})\n",
       "trainValidationSplit: org.apache.spark.ml.tuning.TrainValidationSplit = tvs_4ec7686e2310\n",
       "model: org.apache.spark.ml.tuning.TrainValidationSplitModel = tvs_4ec7686e2310\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val grid = new ParamGridBuilder()\n",
    "    .addGrid(lr.regParam,Array(10e-8,10e-6,10e-4,10e-2))\n",
    "    .build()\n",
    "\n",
    "// val cv = new CrossValidator()\n",
    "//   .setEstimator(pipeline)\n",
    "//   .setEvaluator(evaluator)\n",
    "//   .setEstimatorParamMaps(grid)\n",
    "//   .setNumFolds(5)\n",
    "\n",
    "// val cvModel = cv.fit(df)\n",
    "\n",
    "val trainValidationSplit = new TrainValidationSplit()\n",
    "  .setEstimator(pipeline)\n",
    "  .setEvaluator(evaluator)\n",
    "  .setEstimatorParamMaps(grid)\n",
    "  // 80% of the data will be used for training and the remaining 20% for validation.\n",
    "  .setTrainRatio(0.7)\n",
    "\n",
    "val model = trainValidationSplit.fit(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-----------+--------------------+\n",
      "|                name|final_status|predictions|         probability|\n",
      "+--------------------+------------+-----------+--------------------+\n",
      "|northern lights o...|           0|        0.0|[0.78392009081449...|\n",
      "|happy birthday, m...|           0|        0.0|[0.75705606894479...|\n",
      "|           plexibots|           0|        0.0|[0.70923059165288...|\n",
      "|           sellsafer|           0|        0.0|[0.95349249745577...|\n",
      "|arresting power: ...|           1|        0.0|[0.77689872101372...|\n",
      "|web page to revol...|           0|        0.0|[0.93434627040621...|\n",
      "|        inchicago.co|           0|        1.0|[0.47398301919751...|\n",
      "|project tangled -...|           0|        0.0|[0.73872767245204...|\n",
      "|day of the dead b...|           1|        1.0|[0.24081599891754...|\n",
      "|j.calarese&co mar...|           0|        0.0|[0.91734089576189...|\n",
      "|official elfquest...|           1|        1.0|[0.31152821301541...|\n",
      "|\"kol presents fir...|           0|        0.0|[0.78638534839085...|\n",
      "|world war 3 (canc...|           0|        0.0|[0.86477753391038...|\n",
      "|\"the premiere of ...|           1|        0.0|[0.97143706673729...|\n",
      "|learning a new world|           0|        0.0|[0.91606650012391...|\n",
      "|            eco-pots|           1|        1.0|[0.50066561301782...|\n",
      "|capturing invisib...|           0|        0.0|[0.92301024264567...|\n",
      "|love to love to l...|           0|        0.0|[0.70965427626649...|\n",
      "|lillie monroe aff...|           0|        0.0|[0.92539630244253...|\n",
      "|broken river book...|           1|        1.0|[0.31419064445790...|\n",
      "|sam roark record ...|           0|        0.0|[0.91088907807633...|\n",
      "|a cartoon charact...|           1|        0.0|[0.79189595163778...|\n",
      "|we know clothing ...|           0|        0.0|[0.82967266048801...|\n",
      "|theyburied.us - r...|           0|        0.0|[0.77378004598503...|\n",
      "|naturehd: photo c...|           0|        0.0|[0.78953183531349...|\n",
      "|the mystery of pa...|           0|        0.0|[0.94846134614882...|\n",
      "|         face my car|           0|        0.0|[0.78465884333048...|\n",
      "|in tension :: acc...|           0|        1.0|[0.65589658553672...|\n",
      "|      black the fall|           1|        1.0|[0.60101210638438...|\n",
      "|glass designs (ca...|           0|        0.0|[0.90369059642388...|\n",
      "|the everyday is a...|           0|        0.0|[0.71893677128981...|\n",
      "|upcycled by lilly...|           0|        0.0|[0.86206789226615...|\n",
      "|asmod - jesus lik...|           0|        0.0|[0.87332299933490...|\n",
      "|okinawa kenpo kar...|           0|        0.0|[0.82104002618111...|\n",
      "|dagga wear, cloth...|           0|        0.0|[0.90410767569585...|\n",
      "| west coast cupcakes|           0|        0.0|[0.82762736912337...|\n",
      "|seven points forg...|           0|        0.0|[0.84344235337707...|\n",
      "|  lampworking studio|           0|        0.0|[0.86081361997860...|\n",
      "|\"the \"\"\"\"\"\"\"\"\"\"\"\"...|           1|        0.0|[0.77216252555584...|\n",
      "|robs salsa for a ...|           0|        0.0|[0.71411307195028...|\n",
      "|whatever i want i...|           0|        0.0|[0.90314252610920...|\n",
      "|dragon reign 2.0 ...|           0|        0.0|[0.72014030792185...|\n",
      "|the perfect petit...|           0|        1.0|[0.52743724702967...|\n",
      "|reckon : a street...|           1|        0.0|[0.80092776241229...|\n",
      "|  the holcombe tarot|           1|        0.0|[0.78364532146941...|\n",
      "|but i love you. [...|           1|        0.0|[0.76809679232707...|\n",
      "|help off the trol...|           0|        1.0|[0.48641115180832...|\n",
      "|c-shirt... fabric...|           0|        0.0|[0.80625981131373...|\n",
      "|designing hope fo...|           1|        0.0|[0.76351850136123...|\n",
      "|    shanghai express|           0|        0.0|[0.80252170492322...|\n",
      "|  rawkin' live foods|           0|        0.0|[0.81854029792136...|\n",
      "|bleeding audio: a...|           1|        1.0|[0.45912145611796...|\n",
      "|     delirio furioso|           0|        1.0|[0.53360334133859...|\n",
      "|denim wonder reco...|           1|        1.0|[0.48266241368117...|\n",
      "|snowplow heater m...|           0|        1.0|[0.51402892327945...|\n",
      "|caught in the cam...|           1|        0.0|[0.78889914442186...|\n",
      "|  the wisdom machine|           0|        0.0|[0.71041269543301...|\n",
      "|event cycle 6 - a...|           0|        1.0|[0.54247581184904...|\n",
      "|anonymous: a plac...|           0|        0.0|[0.75679998459661...|\n",
      "|       maths venture|           0|        0.0|[0.84848498408495...|\n",
      "|fithit - hit the ...|           0|        0.0|[0.94310852516959...|\n",
      "|sins and serenity...|           0|        0.0|[0.72707895355066...|\n",
      "|malmös unga foam-...|           0|        0.0|[0.98410920591508...|\n",
      "|gearadelphia: the...|           1|        0.0|[0.82276230803330...|\n",
      "|silent mark -- in...|           0|        1.0|[0.40232246182432...|\n",
      "|        killer curse|           1|        0.0|[0.86352230026472...|\n",
      "|aggressive vocals...|           0|        0.0|[0.92556183556942...|\n",
      "|help me get my li...|           0|        0.0|[0.85062112644416...|\n",
      "|lala & ara: world...|           1|        0.0|[0.81089736834142...|\n",
      "|auto repair video...|           0|        0.0|[0.83875254348367...|\n",
      "|   monkey phone call|           0|        1.0|[0.53454897808485...|\n",
      "|bewilder - a feat...|           0|        1.0|[0.01903304488459...|\n",
      "|the line cutterz ...|           1|        1.0|[0.61463425366660...|\n",
      "|'t.a.p.p.' - some...|           0|        0.0|[0.98608837965200...|\n",
      "|nubarron: the adv...|           0|        0.0|[0.76146870412766...|\n",
      "|thimbleweed park:...|           1|        1.0|[0.32306553805533...|\n",
      "|photographing fan...|           0|        0.0|[0.94006116539813...|\n",
      "|if we can't stop ...|           0|        0.0|[0.72185820980586...|\n",
      "|    sassi girl style|           0|        0.0|[0.86867421458565...|\n",
      "|    martin and marco|           1|        0.0|[0.95321413078787...|\n",
      "|         fox-ception|           0|        0.0|[0.85796060600175...|\n",
      "|look good. do goo...|           0|        0.0|[0.85777488596964...|\n",
      "|baller shoes (sus...|           0|        0.0|[0.94484033411895...|\n",
      "|need coffee roast...|           0|        0.0|[0.85059450706656...|\n",
      "|black swan overni...|           0|        1.0|[0.62337446333388...|\n",
      "|52 tastebud tingl...|           0|        0.0|[0.74784089972023...|\n",
      "|       morning glory|           0|        0.0|[0.89583322004728...|\n",
      "|take a loooong br...|           0|        0.0|[0.80278349017495...|\n",
      "|         slime quest|           0|        1.0|[0.20454191814144...|\n",
      "|   help a ghoul out!|           0|        0.0|[0.81907300499863...|\n",
      "|christopher robin...|           0|        1.0|[0.51556307738267...|\n",
      "|bowlingaverage.co...|           0|        0.0|[0.83501334727959...|\n",
      "|\"m.e.l \"\"\"\"\"\"\"\"\"\"...|           0|        0.0|[0.71902546374212...|\n",
      "|pumpkin online - ...|           1|        1.0|[0.44609870669716...|\n",
      "| nebraska aquaponics|           0|        0.0|[0.91395640791969...|\n",
      "|get vinyl mill re...|           0|        1.0|[0.68432531213141...|\n",
      "|breakfast cult: a...|           1|        1.0|[0.68617587830330...|\n",
      "|    amazeballs balls|           0|        0.0|[0.91536438311585...|\n",
      "|princess zahrizel...|           0|        0.0|[0.85606776639228...|\n",
      "|\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"...|           0|        1.0|[0.66433195475506...|\n",
      "+--------------------+------------+-----------+--------------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// model.bestModel.params.\n",
    "model.bestModel.transform(test).select(\"name\",\"final_status\",\"predictions\",\"probability\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: Double = 0.6468747625444753\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(model.bestModel.transform(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grid2: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tcntVec_fd09521beb46-minDF: 55.0,\n",
       "\tlogreg_7240eeb9b1b1-regParam: 1.0E-7\n",
       "}, {\n",
       "\tcntVec_fd09521beb46-minDF: 75.0,\n",
       "\tlogreg_7240eeb9b1b1-regParam: 1.0E-7\n",
       "}, {\n",
       "\tcntVec_fd09521beb46-minDF: 95.0,\n",
       "\tlogreg_7240eeb9b1b1-regParam: 1.0E-7\n",
       "}, {\n",
       "\tcntVec_fd09521beb46-minDF: 55.0,\n",
       "\tlogreg_7240eeb9b1b1-regParam: 1.0E-5\n",
       "}, {\n",
       "\tcntVec_fd09521beb46-minDF: 75.0,\n",
       "\tlogreg_7240eeb9b1b1-regParam: 1.0E-5\n",
       "}, {\n",
       "\tcntVec_fd09521beb46-minDF: 95.0,\n",
       "\tlogreg_7240eeb9b1b1-regParam: 1.0E-5\n",
       "}, {\n",
       "\tcntVec_fd09521beb46-minDF: 55.0,\n",
       "\tlogreg_7240eeb9b1b1-regParam: 0.001\n",
       "}, {\n",
       "\tcntVec_fd09521beb46-minDF: 75.0,\n",
       "\tlogreg_7240eeb9b1b1-regParam: 0.001\n",
       "}, {\n",
       "\tcntVec_fd09521beb46-minDF: 95.0,\n",
       "\tlogreg_7240eeb9b1b1-regParam: 0.001\n",
       "}, {\n",
       "\tcntVec_fd09521beb46-minDF: 55.0,\n",
       "\tlogreg_7240ee..."
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val grid2 = new ParamGridBuilder()\n",
    "    .addGrid(lr.regParam,Array(10e-8,10e-6,10e-4,10e-2))\n",
    "    .addGrid(cvModel.minDF,Array(55.0,75.0,95.0))\n",
    "    .build()\n",
    "\n",
    "\n",
    "// val cv = new CrossValidator()\n",
    "//   .setEstimator(pipeline)\n",
    "//   .setEvaluator(evaluator)\n",
    "//   .setEstimatorParamMaps(grid)\n",
    "//   .setNumFolds(5)\n",
    "\n",
    "// val cvModel = cv.fit(df)\n",
    "\n",
    "val trainValidationSplit2 = new TrainValidationSplit()\n",
    "  .setEstimator(pipeline)\n",
    "  .setEvaluator(evaluator)\n",
    "  .setEstimatorParamMaps(grid)\n",
    "  // 80% of the data will be used for training and the remaining 20% for validation.\n",
    "  .setTrainRatio(0.7)\n",
    "\n",
    "val modelbis = trainValidationSplit2.fit(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res66: Double = 0.6500708947162648\n"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// modelbis.bestModel\n",
    "evaluator.evaluate(modelbis.bestModel.transform(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res67: Double = 0.6468747625444753\n"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(model.bestModel.transform(test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
