{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://137.194.73.102:4040\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = local[*], app id = local-1573822561448)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world ! from Trainer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.SparkConf\n",
       "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.SparkConf\n",
       "import org.apache.spark.ml.feature.{RegexTokenizer, StopWordsRemover, StringIndexer, CountVectorizer, CountVectorizerModel, VectorAssembler, IDF, OneHotEncoderEstimator}\n",
       "import org.apache.spark.ml.classification.LogisticRegression\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "import org.apache.spark.ml.param.ParamMap\n",
       "conf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@1106517\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@69007186\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.ml.feature.{RegexTokenizer,StopWordsRemover,\n",
    "                                    StringIndexer,CountVectorizer,\n",
    "                                    CountVectorizerModel,VectorAssembler,\n",
    "                                   IDF,OneHotEncoderEstimator}\n",
    "\n",
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n",
    "\n",
    "// import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
    "// import org.apache.spark.ml.evaluation\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "\n",
    "\n",
    "val conf = new SparkConf().setAll(Map(\n",
    "      \"spark.scheduler.mode\" -> \"FIFO\",\n",
    "      \"spark.speculation\" -> \"false\",\n",
    "      \"spark.reducer.maxSizeInFlight\" -> \"48m\",\n",
    "      \"spark.serializer\" -> \"org.apache.spark.serializer.KryoSerializer\",\n",
    "      \"spark.kryoserializer.buffer.max\" -> \"1g\",\n",
    "      \"spark.shuffle.file.buffer\" -> \"32k\",\n",
    "      \"spark.default.parallelism\" -> \"12\",\n",
    "      \"spark.sql.shuffle.partitions\" -> \"12\",\n",
    "      \"spark.driver.maxResultSize\" -> \"2g\"\n",
    "    ))\n",
    "\n",
    "val spark = SparkSession\n",
    "  .builder\n",
    "  .config(conf)\n",
    "  .appName(\"TP Spark : Trainer\")\n",
    "  .getOrCreate()\n",
    "\n",
    "\n",
    "/*******************************************************************************\n",
    "  *\n",
    "  *       TP 3\n",
    "  *\n",
    "  *       - lire le fichier sauvegarder précédemment\n",
    "  *       - construire les Stages du pipeline, puis les assembler\n",
    "  *       - trouver les meilleurs hyperparamètres pour l'entraînement du pipeline avec une grid-search\n",
    "  *       - Sauvegarder le pipeline entraîné\n",
    "  *\n",
    "  *       if problems with unimported modules => sbt plugins update\n",
    "  *\n",
    "  ********************************************************************************/\n",
    "\n",
    "println(\"hello world ! from Trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 12 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df:DataFrame = spark.read.parquet(\"/home/jorge/Documents/Cours/Spark/RepoAdotTPs/data/prepared_trainingset/\")\n",
    "\n",
    "// df.select(\"project_id\", \"name\", \"desc\", \"goal\").show(5)\n",
    "// df.select(\"keywords\", \"final_status\", \"country2\", \"currency2\").show(5)\n",
    "// df.select(\"deadline2\", \"created_at2\", \"launched_at2\", \"days_campaign\").show(5)\n",
    "// df.select(\"hours_prepa\", \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer: org.apache.spark.ml.feature.RegexTokenizer = regexTok_10225ad89bea\n",
       "remover: org.apache.spark.ml.feature.StopWordsRemover = stopWords_e62469a602ad\n",
       "cvModel: org.apache.spark.ml.feature.CountVectorizer = cntVec_163225298fad\n",
       "idf: org.apache.spark.ml.feature.IDF = idf_45fbefa93475\n",
       "indexerCountry: org.apache.spark.ml.feature.StringIndexer = strIdx_1dac9ea4dfb8\n",
       "indexerCurrency: org.apache.spark.ml.feature.StringIndexer = strIdx_0f9fb213412b\n",
       "encoder: org.apache.spark.ml.feature.OneHotEncoderEstimator = oneHotEncoder_bb0c386815dd\n",
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_bc3574af7f54\n",
       "lr: org.apache.spark.ml.classification.LogisticRegression = logreg_1c3d4522bf2f\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tokenizer = new RegexTokenizer()\n",
    "  .setPattern(\"\\\\W+\")\n",
    "  .setGaps(true)\n",
    "  .setInputCol(\"text\")\n",
    "  .setOutputCol(\"tokens\")\n",
    "// val dfTokenized = tokenizer.transform(df)\n",
    "\n",
    "\n",
    "val remover = new StopWordsRemover()\n",
    "  .setInputCol(tokenizer.getOutputCol)\n",
    "  .setOutputCol(\"filtered\")\n",
    "\n",
    "// val dfsw = remover.transform(dfTokenized)\n",
    " \n",
    "\n",
    "val cvModel: CountVectorizer = new CountVectorizer()\n",
    "    .setInputCol(remover.getOutputCol)\n",
    "    .setOutputCol(\"vect\")\n",
    "    .setMinDF(50)\n",
    "    \n",
    "    \n",
    "\n",
    "// val dfv = cvModel.fit(dfsw).transform(dfsw)\n",
    "\n",
    "\n",
    "\n",
    "val idf = new IDF()\n",
    "    .setInputCol(cvModel.getOutputCol)\n",
    "    .setOutputCol(\"tfidf\")\n",
    "\n",
    "// val idfModel = idf.fit(dfv)\n",
    "\n",
    "// val rescaledData = idfModel.transform(dfv)\n",
    "                                      \n",
    "\n",
    "val indexerCountry = new StringIndexer()\n",
    "  .setInputCol(\"country2\")\n",
    "  .setOutputCol(\"country_indexed\")\n",
    "\n",
    "val indexerCurrency = new StringIndexer()\n",
    "  .setInputCol(\"currency2\")\n",
    "  .setOutputCol(\"currency_indexed\")\n",
    "\n",
    "\n",
    "// val indexedCountry = indexerCountry.fit(rescaledData).transform(rescaledData)\n",
    "// val indexedCountryCurrency = indexerCurrency.fit(indexedCountry).transform(indexedCountry)\n",
    "\n",
    "\n",
    "val encoder = new OneHotEncoderEstimator()\n",
    "  .setInputCols(Array(\"country_indexed\", \"currency_indexed\"))\n",
    "  .setOutputCols(Array(\"country_onehot\", \"currency_onehot\"))\n",
    "\n",
    "// val model = encoder.fit(indexedCountryCurrency)\n",
    "// val encoded = model.transform(indexedCountryCurrency)\n",
    "\n",
    "\n",
    "val assembler = new VectorAssembler()\n",
    "  .setInputCols(Array(\"tfidf\",\"days_campaign\",\"hours_prepa\",\"goal\",\"country_onehot\",\"currency_onehot\"))\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "val lr = new LogisticRegression()\n",
    "  .setElasticNetParam(0.0)\n",
    "  .setFitIntercept(true)\n",
    "  .setFeaturesCol(\"features\")\n",
    "  .setLabelCol(\"final_status\")\n",
    "  .setStandardization(true)\n",
    "  .setPredictionCol(\"predictions\")\n",
    "  .setRawPredictionCol(\"raw_predictions\")\n",
    "  .setThresholds(Array(0.7, 0.3))\n",
    "  .setTol(1.0e-6)\n",
    "  .setMaxIter(20)\n",
    "\n",
    "// val transformed = assembler\n",
    "//     .setHandleInvalid(\"skip\")\n",
    "//     .transform(encoded)\n",
    "//     .drop(\"project_id\",\"name\",\"desc\",\"goal\",\"keywords\",\n",
    "//          \"country2\",\"currency2\",\"deadline2\",\"created_at2\",\"launched_at2\",\n",
    "//          \"days_campaign\",\"hours_prepa\",\"text\",\"tokens\",\"filtered\",\"vect\",\n",
    "//          \"country_indexed\",\"currency_indexed\",\"tfidf\",\"days_campaign\",\n",
    "//           \"hours_prepa\",\"goal\",\"country_onehot\",\"currency_onehot\")\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_708c23f7cfe5\n",
       "model: org.apache.spark.ml.PipelineModel = pipeline_708c23f7cfe5\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// transformed.show(numRows=5,truncate=false)\n",
    "\n",
    "val pipeline = new Pipeline()\n",
    "  .setStages(Array(tokenizer, remover,cvModel,idf,indexerCountry,\n",
    "                  indexerCurrency,encoder, assembler,lr ))\n",
    "\n",
    "// Fit the pipeline to training documents.\n",
    "val model = pipeline.fit(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-----+\n",
      "|final_status|predictions|count|\n",
      "+------------+-----------+-----+\n",
      "|           1|        0.0| 8415|\n",
      "|           0|        1.0|26141|\n",
      "|           1|        1.0|25978|\n",
      "|           0|        0.0|47080|\n",
      "+------------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(df).columns\n",
    "\n",
    "// model.transform(df).select(\"text\",\"predictions\",\"final_status\")\n",
    "model.transform(df).groupBy(\"final_status\", \"predictions\").count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [project_id: string, name: string ... 12 more fields]\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [project_id: string, name: string ... 12 more fields]\n",
       "size: (Long, Long) = (96968,10646)\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(train,test) = df.randomSplit(Array[Double](0.9, 0.1))\n",
    "val size = (train.count,test.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model2: org.apache.spark.ml.PipelineModel = pipeline_a83e74e46e29\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val model2 = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+--------------------+\n",
      "|final_status|predictions|         probability|\n",
      "+------------+-----------+--------------------+\n",
      "|           1|        0.0|[0.83346856290985...|\n",
      "|           0|        0.0|[0.75592356170283...|\n",
      "|           0|        0.0|[0.88582257302640...|\n",
      "|           0|        1.0|[0.43505188248873...|\n",
      "|           1|        0.0|[0.72957488453949...|\n",
      "|           0|        0.0|[0.92849923107638...|\n",
      "|           0|        0.0|[0.81460799809196...|\n",
      "|           0|        1.0|[0.59479358906910...|\n",
      "|           1|        0.0|[0.71505324314113...|\n",
      "|           0|        0.0|[0.96793421536937...|\n",
      "|           0|        1.0|[0.63416940279811...|\n",
      "|           0|        1.0|[0.65282514269261...|\n",
      "|           0|        1.0|[0.58268471533627...|\n",
      "|           1|        0.0|[0.72290322572450...|\n",
      "|           0|        1.0|[0.66616489673114...|\n",
      "|           1|        1.0|[0.37431144811710...|\n",
      "|           0|        1.0|[0.63116156355414...|\n",
      "|           0|        0.0|[0.85599357146677...|\n",
      "|           0|        1.0|[0.68872860703926...|\n",
      "|           0|        1.0|[0.67228491488633...|\n",
      "|           0|        1.0|[0.49732592387325...|\n",
      "|           0|        0.0|[0.81913936468613...|\n",
      "|           0|        0.0|[0.93537069323220...|\n",
      "|           0|        0.0|[0.91213233960820...|\n",
      "|           1|        0.0|[0.70993240331678...|\n",
      "|           1|        1.0|[0.54222050210080...|\n",
      "|           0|        1.0|[0.68730882446157...|\n",
      "|           0|        0.0|[0.90997210435471...|\n",
      "|           1|        0.0|[0.78487702951666...|\n",
      "|           0|        0.0|[0.86561819295021...|\n",
      "|           1|        0.0|[0.84479938743683...|\n",
      "|           0|        0.0|[0.87202304723893...|\n",
      "|           0|        0.0|[0.76966524648818...|\n",
      "|           0|        0.0|[0.83202318963003...|\n",
      "|           0|        0.0|[0.93005195192951...|\n",
      "|           0|        1.0|[0.68277032124212...|\n",
      "|           0|        0.0|[0.92532451372925...|\n",
      "|           0|        0.0|[0.98541966979351...|\n",
      "|           0|        0.0|[0.98363413841076...|\n",
      "|           0|        0.0|[0.89534541711688...|\n",
      "|           0|        1.0|[0.41853512176111...|\n",
      "|           0|        1.0|[0.61813341739530...|\n",
      "|           1|        0.0|[0.95264843235898...|\n",
      "|           0|        0.0|[0.71835472690342...|\n",
      "|           1|        1.0|[0.59072761648209...|\n",
      "|           0|        0.0|[0.75936597512115...|\n",
      "|           1|        0.0|[0.86089586582737...|\n",
      "|           0|        0.0|[0.90138737953208...|\n",
      "|           0|        1.0|[0.30153361926260...|\n",
      "|           0|        0.0|[0.81040952400868...|\n",
      "|           1|        1.0|[0.49396438467748...|\n",
      "|           0|        1.0|[0.67423807548902...|\n",
      "|           1|        1.0|[0.46775108350302...|\n",
      "|           0|        1.0|[0.62787599762290...|\n",
      "|           0|        0.0|[0.75488504120632...|\n",
      "|           0|        0.0|[0.82200489541657...|\n",
      "|           0|        0.0|[0.82363182448105...|\n",
      "|           0|        0.0|[0.71104531579740...|\n",
      "|           0|        1.0|[0.62973239995235...|\n",
      "|           0|        0.0|[0.78258160399008...|\n",
      "|           0|        0.0|[0.71946128471528...|\n",
      "|           0|        0.0|[0.84117861348247...|\n",
      "|           0|        1.0|[0.62574988852544...|\n",
      "|           0|        0.0|[0.74856077927003...|\n",
      "|           0|        0.0|[0.76749059244363...|\n",
      "|           1|        0.0|[0.82707646852458...|\n",
      "|           0|        0.0|[0.81710109143861...|\n",
      "|           0|        0.0|[0.90549051263769...|\n",
      "|           0|        0.0|[0.85966305864867...|\n",
      "|           0|        1.0|[0.51271365350859...|\n",
      "|           0|        0.0|[0.70465515434161...|\n",
      "|           0|        0.0|[0.99999206963391...|\n",
      "|           0|        1.0|[0.47858866080222...|\n",
      "|           1|        1.0|[0.35726262222787...|\n",
      "|           0|        1.0|[0.55158005489001...|\n",
      "|           0|        0.0|[0.91722655821560...|\n",
      "|           0|        1.0|[0.59934767024224...|\n",
      "|           0|        0.0|[0.93964834575800...|\n",
      "|           0|        0.0|[0.79149615619929...|\n",
      "|           0|        0.0|[0.70870128439043...|\n",
      "|           0|        1.0|[0.55350442677648...|\n",
      "|           0|        0.0|[0.89362372326998...|\n",
      "|           0|        0.0|[0.95292123353798...|\n",
      "|           0|        0.0|[0.81150529839643...|\n",
      "|           0|        0.0|[0.99999021232065...|\n",
      "|           0|        1.0|[0.34591235321407...|\n",
      "|           0|        0.0|[0.75769274775381...|\n",
      "|           0|        1.0|[0.17656568178005...|\n",
      "|           0|        0.0|[0.85937709168240...|\n",
      "|           0|        1.0|[0.58884994727052...|\n",
      "|           1|        1.0|[0.26183249077615...|\n",
      "|           1|        1.0|[0.65138939369951...|\n",
      "|           1|        1.0|[0.57079436721088...|\n",
      "|           1|        1.0|[0.33214393212590...|\n",
      "|           1|        1.0|[0.65515034022308...|\n",
      "|           0|        0.0|[0.93972320214194...|\n",
      "|           0|        0.0|[0.81287209852072...|\n",
      "|           1|        1.0|[0.44355639747427...|\n",
      "|           0|        0.0|[0.73070756360647...|\n",
      "|           1|        0.0|[0.83606882019091...|\n",
      "+------------+-----------+--------------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 24 more fields]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val predictions = model2.transform(test)\n",
    "\n",
    "predictions.select(\"final_status\",\"predictions\",\"probability\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "// // Clear the prediction threshold so the model will return probabilities\n",
    "// model2.clearThreshold\n",
    "\n",
    "// // Compute raw scores on the test set\n",
    "// val predictionAndLabels = test.map { case LabeledPoint(label, features) =>\n",
    "//   val prediction = model2.predict(features)\n",
    "//   (prediction, label)\n",
    "// }\n",
    "\n",
    "// // Instantiate metrics object\n",
    "// val metrics = new BinaryClassificationMetrics(predictionAndLabels)\n",
    "\n",
    "// // Precision by threshold\n",
    "// val precision = metrics.precisionByThreshold\n",
    "// precision.foreach { case (t, p) =>\n",
    "//   println(s\"Threshold: $t, Precision: $p\")\n",
    "// }\n",
    "\n",
    "// // Recall by threshold\n",
    "// val recall = metrics.recallByThreshold\n",
    "// recall.foreach { case (t, r) =>\n",
    "//   println(s\"Threshold: $t, Recall: $r\")\n",
    "// }\n",
    "\n",
    "// // Precision-Recall Curve\n",
    "// val PRC = metrics.pr\n",
    "\n",
    "// // F-measure\n",
    "// val f1Score = metrics.fMeasureByThreshold\n",
    "// f1Score.foreach { case (t, f) =>\n",
    "//   println(s\"Threshold: $t, F-score: $f, Beta = 1\")\n",
    "// }\n",
    "\n",
    "// val beta = 0.5\n",
    "// val fScore = metrics.fMeasureByThreshold(beta)\n",
    "// f1Score.foreach { case (t, f) =>\n",
    "//   println(s\"Threshold: $t, F-score: $f, Beta = 0.5\")\n",
    "// }\n",
    "\n",
    "// // AUPRC\n",
    "// val auPRC = metrics.areaUnderPR\n",
    "// println(s\"Area under precision-recall curve = $auPRC\")\n",
    "\n",
    "// // Compute thresholds used in ROC and PR curves\n",
    "// val thresholds = precision.map(_._1)\n",
    "\n",
    "// // ROC Curve\n",
    "// val roc = metrics.roc\n",
    "\n",
    "// // AUROC\n",
    "// val auROC = metrics.areaUnderROC\n",
    "// println(s\"Area under ROC = $auROC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy = 0.6560841473369508\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_85715d799e77\n",
       "f1: Double = 0.6560841473369508\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "      .setLabelCol(\"final_status\")\n",
    "      .setPredictionCol(\"predictions\")\n",
    "      .setMetricName(\"f1\")\n",
    "\n",
    "val f1 = evaluator.evaluate(predictions)\n",
    "println(\"Test set accuracy = \" + f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder, CrossValidatorModel, TrainValidationSplit}\n",
       "import org.apache.spark.ml.param.ParamMap\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder,CrossValidatorModel,TrainValidationSplit}\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tlogreg_1c3d4522bf2f-regParam: 1.0E-7\n",
       "}, {\n",
       "\tlogreg_1c3d4522bf2f-regParam: 1.0E-5\n",
       "}, {\n",
       "\tlogreg_1c3d4522bf2f-regParam: 0.001\n",
       "}, {\n",
       "\tlogreg_1c3d4522bf2f-regParam: 0.1\n",
       "})\n",
       "trainValidationSplit: org.apache.spark.ml.tuning.TrainValidationSplit = tvs_5fc8a7d699d5\n",
       "model: org.apache.spark.ml.tuning.TrainValidationSplitModel = tvs_5fc8a7d699d5\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val grid = new ParamGridBuilder()\n",
    "    .addGrid(lr.regParam,Array(10e-8,10e-6,10e-4,10e-2))\n",
    "    .build()\n",
    "\n",
    "// val cv = new CrossValidator()\n",
    "//   .setEstimator(pipeline)\n",
    "//   .setEvaluator(evaluator)\n",
    "//   .setEstimatorParamMaps(grid)\n",
    "//   .setNumFolds(5)\n",
    "\n",
    "// val cvModel = cv.fit(df)\n",
    "\n",
    "val trainValidationSplit = new TrainValidationSplit()\n",
    "  .setEstimator(pipeline)\n",
    "  .setEvaluator(evaluator)\n",
    "  .setEstimatorParamMaps(grid)\n",
    "  // 80% of the data will be used for training and the remaining 20% for validation.\n",
    "  .setTrainRatio(0.7)\n",
    "\n",
    "val model = trainValidationSplit.fit(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-----------+--------------------+\n",
      "|                name|final_status|predictions|         probability|\n",
      "+--------------------+------------+-----------+--------------------+\n",
      "|northern lights o...|           0|        0.0|[0.78392009081449...|\n",
      "|happy birthday, m...|           0|        0.0|[0.75705606894479...|\n",
      "|           plexibots|           0|        0.0|[0.70923059165288...|\n",
      "|           sellsafer|           0|        0.0|[0.95349249745577...|\n",
      "|arresting power: ...|           1|        0.0|[0.77689872101372...|\n",
      "|web page to revol...|           0|        0.0|[0.93434627040621...|\n",
      "|        inchicago.co|           0|        1.0|[0.47398301919751...|\n",
      "|project tangled -...|           0|        0.0|[0.73872767245204...|\n",
      "|day of the dead b...|           1|        1.0|[0.24081599891754...|\n",
      "|j.calarese&co mar...|           0|        0.0|[0.91734089576189...|\n",
      "|official elfquest...|           1|        1.0|[0.31152821301541...|\n",
      "|\"kol presents fir...|           0|        0.0|[0.78638534839085...|\n",
      "|world war 3 (canc...|           0|        0.0|[0.86477753391038...|\n",
      "|\"the premiere of ...|           1|        0.0|[0.97143706673729...|\n",
      "|learning a new world|           0|        0.0|[0.91606650012391...|\n",
      "|            eco-pots|           1|        1.0|[0.50066561301782...|\n",
      "|capturing invisib...|           0|        0.0|[0.92301024264567...|\n",
      "|love to love to l...|           0|        0.0|[0.70965427626649...|\n",
      "|lillie monroe aff...|           0|        0.0|[0.92539630244253...|\n",
      "|broken river book...|           1|        1.0|[0.31419064445790...|\n",
      "|sam roark record ...|           0|        0.0|[0.91088907807633...|\n",
      "|a cartoon charact...|           1|        0.0|[0.79189595163778...|\n",
      "|we know clothing ...|           0|        0.0|[0.82967266048801...|\n",
      "|theyburied.us - r...|           0|        0.0|[0.77378004598503...|\n",
      "|naturehd: photo c...|           0|        0.0|[0.78953183531349...|\n",
      "|the mystery of pa...|           0|        0.0|[0.94846134614882...|\n",
      "|         face my car|           0|        0.0|[0.78465884333048...|\n",
      "|in tension :: acc...|           0|        1.0|[0.65589658553672...|\n",
      "|      black the fall|           1|        1.0|[0.60101210638438...|\n",
      "|glass designs (ca...|           0|        0.0|[0.90369059642388...|\n",
      "|the everyday is a...|           0|        0.0|[0.71893677128981...|\n",
      "|upcycled by lilly...|           0|        0.0|[0.86206789226615...|\n",
      "|asmod - jesus lik...|           0|        0.0|[0.87332299933490...|\n",
      "|okinawa kenpo kar...|           0|        0.0|[0.82104002618111...|\n",
      "|dagga wear, cloth...|           0|        0.0|[0.90410767569585...|\n",
      "| west coast cupcakes|           0|        0.0|[0.82762736912337...|\n",
      "|seven points forg...|           0|        0.0|[0.84344235337707...|\n",
      "|  lampworking studio|           0|        0.0|[0.86081361997860...|\n",
      "|\"the \"\"\"\"\"\"\"\"\"\"\"\"...|           1|        0.0|[0.77216252555584...|\n",
      "|robs salsa for a ...|           0|        0.0|[0.71411307195028...|\n",
      "|whatever i want i...|           0|        0.0|[0.90314252610920...|\n",
      "|dragon reign 2.0 ...|           0|        0.0|[0.72014030792185...|\n",
      "|the perfect petit...|           0|        1.0|[0.52743724702967...|\n",
      "|reckon : a street...|           1|        0.0|[0.80092776241229...|\n",
      "|  the holcombe tarot|           1|        0.0|[0.78364532146941...|\n",
      "|but i love you. [...|           1|        0.0|[0.76809679232707...|\n",
      "|help off the trol...|           0|        1.0|[0.48641115180832...|\n",
      "|c-shirt... fabric...|           0|        0.0|[0.80625981131373...|\n",
      "|designing hope fo...|           1|        0.0|[0.76351850136123...|\n",
      "|    shanghai express|           0|        0.0|[0.80252170492322...|\n",
      "|  rawkin' live foods|           0|        0.0|[0.81854029792136...|\n",
      "|bleeding audio: a...|           1|        1.0|[0.45912145611796...|\n",
      "|     delirio furioso|           0|        1.0|[0.53360334133859...|\n",
      "|denim wonder reco...|           1|        1.0|[0.48266241368117...|\n",
      "|snowplow heater m...|           0|        1.0|[0.51402892327945...|\n",
      "|caught in the cam...|           1|        0.0|[0.78889914442186...|\n",
      "|  the wisdom machine|           0|        0.0|[0.71041269543301...|\n",
      "|event cycle 6 - a...|           0|        1.0|[0.54247581184904...|\n",
      "|anonymous: a plac...|           0|        0.0|[0.75679998459661...|\n",
      "|       maths venture|           0|        0.0|[0.84848498408495...|\n",
      "|fithit - hit the ...|           0|        0.0|[0.94310852516959...|\n",
      "|sins and serenity...|           0|        0.0|[0.72707895355066...|\n",
      "|malmös unga foam-...|           0|        0.0|[0.98410920591508...|\n",
      "|gearadelphia: the...|           1|        0.0|[0.82276230803330...|\n",
      "|silent mark -- in...|           0|        1.0|[0.40232246182432...|\n",
      "|        killer curse|           1|        0.0|[0.86352230026472...|\n",
      "|aggressive vocals...|           0|        0.0|[0.92556183556942...|\n",
      "|help me get my li...|           0|        0.0|[0.85062112644416...|\n",
      "|lala & ara: world...|           1|        0.0|[0.81089736834142...|\n",
      "|auto repair video...|           0|        0.0|[0.83875254348367...|\n",
      "|   monkey phone call|           0|        1.0|[0.53454897808485...|\n",
      "|bewilder - a feat...|           0|        1.0|[0.01903304488459...|\n",
      "|the line cutterz ...|           1|        1.0|[0.61463425366660...|\n",
      "|'t.a.p.p.' - some...|           0|        0.0|[0.98608837965200...|\n",
      "|nubarron: the adv...|           0|        0.0|[0.76146870412766...|\n",
      "|thimbleweed park:...|           1|        1.0|[0.32306553805533...|\n",
      "|photographing fan...|           0|        0.0|[0.94006116539813...|\n",
      "|if we can't stop ...|           0|        0.0|[0.72185820980586...|\n",
      "|    sassi girl style|           0|        0.0|[0.86867421458565...|\n",
      "|    martin and marco|           1|        0.0|[0.95321413078787...|\n",
      "|         fox-ception|           0|        0.0|[0.85796060600175...|\n",
      "|look good. do goo...|           0|        0.0|[0.85777488596964...|\n",
      "|baller shoes (sus...|           0|        0.0|[0.94484033411895...|\n",
      "|need coffee roast...|           0|        0.0|[0.85059450706656...|\n",
      "|black swan overni...|           0|        1.0|[0.62337446333388...|\n",
      "|52 tastebud tingl...|           0|        0.0|[0.74784089972023...|\n",
      "|       morning glory|           0|        0.0|[0.89583322004728...|\n",
      "|take a loooong br...|           0|        0.0|[0.80278349017495...|\n",
      "|         slime quest|           0|        1.0|[0.20454191814144...|\n",
      "|   help a ghoul out!|           0|        0.0|[0.81907300499863...|\n",
      "|christopher robin...|           0|        1.0|[0.51556307738267...|\n",
      "|bowlingaverage.co...|           0|        0.0|[0.83501334727959...|\n",
      "|\"m.e.l \"\"\"\"\"\"\"\"\"\"...|           0|        0.0|[0.71902546374212...|\n",
      "|pumpkin online - ...|           1|        1.0|[0.44609870669716...|\n",
      "| nebraska aquaponics|           0|        0.0|[0.91395640791969...|\n",
      "|get vinyl mill re...|           0|        1.0|[0.68432531213141...|\n",
      "|breakfast cult: a...|           1|        1.0|[0.68617587830330...|\n",
      "|    amazeballs balls|           0|        0.0|[0.91536438311585...|\n",
      "|princess zahrizel...|           0|        0.0|[0.85606776639228...|\n",
      "|\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"...|           0|        1.0|[0.66433195475506...|\n",
      "+--------------------+------------+-----------+--------------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// model.bestModel.params.\n",
    "model.bestModel.transform(test).select(\"name\",\"final_status\",\"predictions\",\"probability\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: Double = 0.6468747625444753\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(model.bestModel.transform(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grid2: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tcntVec_fd09521beb46-minDF: 55.0,\n",
       "\tlogreg_7240eeb9b1b1-regParam: 1.0E-7\n",
       "}, {\n",
       "\tcntVec_fd09521beb46-minDF: 75.0,\n",
       "\tlogreg_7240eeb9b1b1-regParam: 1.0E-7\n",
       "}, {\n",
       "\tcntVec_fd09521beb46-minDF: 95.0,\n",
       "\tlogreg_7240eeb9b1b1-regParam: 1.0E-7\n",
       "}, {\n",
       "\tcntVec_fd09521beb46-minDF: 55.0,\n",
       "\tlogreg_7240eeb9b1b1-regParam: 1.0E-5\n",
       "}, {\n",
       "\tcntVec_fd09521beb46-minDF: 75.0,\n",
       "\tlogreg_7240eeb9b1b1-regParam: 1.0E-5\n",
       "}, {\n",
       "\tcntVec_fd09521beb46-minDF: 95.0,\n",
       "\tlogreg_7240eeb9b1b1-regParam: 1.0E-5\n",
       "}, {\n",
       "\tcntVec_fd09521beb46-minDF: 55.0,\n",
       "\tlogreg_7240eeb9b1b1-regParam: 0.001\n",
       "}, {\n",
       "\tcntVec_fd09521beb46-minDF: 75.0,\n",
       "\tlogreg_7240eeb9b1b1-regParam: 0.001\n",
       "}, {\n",
       "\tcntVec_fd09521beb46-minDF: 95.0,\n",
       "\tlogreg_7240eeb9b1b1-regParam: 0.001\n",
       "}, {\n",
       "\tcntVec_fd09521beb46-minDF: 55.0,\n",
       "\tlogreg_7240ee..."
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val grid2 = new ParamGridBuilder()\n",
    "    .addGrid(lr.regParam,Array(10e-8,10e-6,10e-4,10e-2))\n",
    "    .addGrid(cvModel.minDF,Array(55.0,75.0,95.0))\n",
    "    .build()\n",
    "\n",
    "\n",
    "// val cv = new CrossValidator()\n",
    "//   .setEstimator(pipeline)\n",
    "//   .setEvaluator(evaluator)\n",
    "//   .setEstimatorParamMaps(grid)\n",
    "//   .setNumFolds(5)\n",
    "\n",
    "// val cvModel = cv.fit(df)\n",
    "\n",
    "val trainValidationSplit2 = new TrainValidationSplit()\n",
    "  .setEstimator(pipeline)\n",
    "  .setEvaluator(evaluator)\n",
    "  .setEstimatorParamMaps(grid)\n",
    "  // 80% of the data will be used for training and the remaining 20% for validation.\n",
    "  .setTrainRatio(0.7)\n",
    "\n",
    "val modelbis = trainValidationSplit2.fit(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res66: Double = 0.6500708947162648\n"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// modelbis.bestModel\n",
    "evaluator.evaluate(modelbis.bestModel.transform(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res67: Double = 0.6468747625444753\n"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(model.bestModel.transform(test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
