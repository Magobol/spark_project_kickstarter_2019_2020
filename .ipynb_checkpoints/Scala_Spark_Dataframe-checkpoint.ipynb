{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://192.168.1.88:4040\n",
       "SparkContext available as 'sc' (version = 2.4.4, master = local[*], app id = local-1575311985253)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.SparkConf\n",
       "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
       "conf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@4719d0b6\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@64a23c0c\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "\n",
    "val conf = {new SparkConf().setAll(Map(\"spark.scheduler.mode\" -> \"FIFO\",\n",
    "      \"spark.speculation\" -> \"false\",\n",
    "      \"spark.reducer.maxSizeInFlight\" -> \"48m\",\n",
    "      \"spark.serializer\" -> \"org.apache.spark.serializer.KryoSerializer\",\n",
    "      \"spark.kryoserializer.buffer.max\" -> \"1g\",\n",
    "      \"spark.shuffle.file.buffer\" -> \"32k\",\n",
    "      \"spark.default.parallelism\" -> \"12\",\n",
    "      \"spark.sql.shuffle.partitions\" -> \"12\"\n",
    "    ))}\n",
    "\n",
    "    // Initialisation du SparkSession qui est le point d'entrée vers Spark SQL (donne accès aux dataframes, aux RDD,\n",
    "    // création de tables temporaires, etc., et donc aux mécanismes de distribution des calculs)\n",
    "val spark = {SparkSession\n",
    "  .builder\n",
    "  .config(conf)\n",
    "  .appName(\"TP Spark : Preprocessor\")\n",
    "  .getOrCreate}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de lignes : 108129\n",
      "Nombre de colonnes : 14\n",
      "\n",
      "\n",
      "Hello World ! from Preprocessor\n",
      "\n",
      "\n",
      "+--------+-----+\n",
      "|currency|count|\n",
      "+--------+-----+\n",
      "|     NOK|  113|\n",
      "|     DKK|  190|\n",
      "|     SEK|  230|\n",
      "|     NZD|  348|\n",
      "|     EUR|  792|\n",
      "|     AUD| 1849|\n",
      "|     CAD| 3515|\n",
      "|     GBP| 8381|\n",
      "|     USD|85515|\n",
      "+--------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 12 more fields]\n",
       "cleaner: String => String = <function1>\n",
       "udf_c: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,StringType,Some(List(StringType)))\n",
       "dfCasted: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 9 more fields]\n"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df:DataFrame = spark\n",
    "      .read\n",
    "      .option(\"header\", true) \n",
    "      .option(\"inferSchema\", \"true\") // pour inférer le type de chaque colonne (Int, String, etc.)\n",
    "      .csv(\"/home/jorge/Documents/Git/spark_project_kickstarter_2019_2020/data/train_clean.csv\")\n",
    "\n",
    "println(s\"Nombre de lignes : ${df.count}\")\n",
    "println(s\"Nombre de colonnes : ${df.columns.length}\")\n",
    "println(\"\\n\")\n",
    "println(\"Hello World ! from Preprocessor\")\n",
    "println(\"\\n\")\n",
    "\n",
    "// val dfcasted:DataFrame = df\n",
    "//     .withColumn(\"goal\",$\"goal\".cast(\"Int\"))\n",
    "//     .withColumn(\"deadline\",$\"deadline\".cast(\"Int\"))\n",
    "//     .withColumn(\"state_changed_at\",$\"state_changed_at\".cast(\"Int\"))\n",
    "//     .withColumn(\"created_at\",$\"created_at\".cast(\"Int\"))\n",
    "//     .withColumn(\"launched_at\", $\"launched_at\".cast(\"Int\"))\n",
    "//     .withColumn(\"backers_count\", $\"backers_count\".cast(\"Int\"))\n",
    "//     .withColumn(\"final_status\", $\"final_status\".cast(\"Int\"))\n",
    "\n",
    "val cleaner = (s:String)=>{s.replaceAll(\",\",\"_\")}\n",
    "val udf_c = udf(cleaner)\n",
    "\n",
    "\n",
    "val dfCasted: DataFrame = df\n",
    "    .withColumn(\"goal\", $\"goal\".cast(\"Int\"))\n",
    "    .withColumn(\"deadline\" , $\"deadline\".cast(\"Int\"))\n",
    "    .withColumn(\"state_changed_at\", $\"state_changed_at\".cast(\"Int\"))\n",
    "    .withColumn(\"created_at\", $\"created_at\".cast(\"Int\"))\n",
    "    .withColumn(\"launched_at\", $\"launched_at\".cast(\"Int\"))\n",
    "    .withColumn(\"backers_count\", $\"backers_count\".cast(\"Int\"))\n",
    "    .withColumn(\"final_status\", $\"final_status\".cast(\"Int\"))\n",
    "    .withColumn(\"name\",udf_c($\"name\"))\n",
    "    .dropDuplicates(\"deadline\")\n",
    "    .filter(!isnull($\"state_changed_at\"))\n",
    "    .withColumn(\"country\",when($\"country\" === \"False\",$\"currency\").otherwise($\"country\"))\n",
    "    .filter(($\"disable_communication\"===\"True\") || ($\"disable_communication\"===\"False\"))\n",
    "    .drop(\"disable_communication\")\n",
    "    .filter($\"country\" rlike \".{2}\")\n",
    "    .filter($\"currency\" rlike \".{3}\")\n",
    "    .drop(\"backers_count\",\"state_changed_at\")\n",
    "    .withColumn(\"days_campaign\",datediff(from_unixtime($\"deadline\"),from_unixtime($\"launched_at\")))\n",
    "    .withColumn(\"hours_prepa\",(($\"launched_at\"-$\"created_at\")/60).cast(\"Int\"))\n",
    "    .drop(\"launched_at\",\"deadline\",\"created_at\")\n",
    "    .withColumn(\"name\",lower($\"name\"))\n",
    "    .withColumn(\"desc\",lower($\"desc\"))\n",
    "    .withColumn(\"keywords\",lower($\"keywords\"))\n",
    "    .withColumn(\"text\",concat($\"name\",lit(\" \"),$\"desc\",lit(\" \"),$\"keywords\"))\n",
    "    .withColumn(\"days_campaign\",when(isnull($\"days_campaign\"),-1).otherwise($\"days_campaign\"))\n",
    "    .withColumn(\"hours_prepa\",when(isnull($\"hours_prepa\"),-1).otherwise($\"hours_prepa\"))\n",
    "    .withColumn(\"goal\",when(isnull($\"goal\"),-1).otherwise($\"goal\"))\n",
    "    .withColumn(\"country\",when(isnull($\"country\"),\" \").otherwise($\"country\"))\n",
    "    .withColumn(\"currency\",when(isnull($\"currency\"),\" \").otherwise($\"currency\"))\n",
    "\n",
    "\n",
    "// df3.write.parquet(\"/home/jorge/Documents/Git/spark_project_kickstarter_2019_2020/cleanData.parquet\")\n",
    "// df.select($\"goal\").filter(col(\"goal\").isNull).show\n",
    "\n",
    "// dfCasted.select(\"name\",\"goal\", \"backers_count\", \"final_status\").describe().show\n",
    "// dfCasted.groupBy(\"deadline\").count.orderBy($\"count\".desc).show(100)\n",
    "\n",
    "dfCasted.select(\"country\",\"currency\").groupBy(\"currency\").count.orderBy(\"count\").show\n",
    "// dfCasted.select(\"name\",\"desc\",\"goal\",\"keywords\",\"currency\").filter(isnull($\"country\")).show(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res69: Array[String] = Array(project_id, name, desc, goal, keywords, disable_communication, country, currency, deadline, state_changed_at, created_at, launched_at, backers_count, final_status)\n"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfCasted.columns\n",
    "// dfCasted.groupBy(\"project_id\").count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " cannot resolve '`backers_count`' given input columns: [hours_prepa, keywords, days_campaign, project_id, currency, goal, name, desc, final_status, country, text];;",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: cannot resolve '`backers_count`' given input columns: [hours_prepa, keywords, days_campaign, project_id, currency, goal, name, desc, final_status, country, text];;",
      "'Project [goal#1480, 'backers_count, final_status#1320]",
      "+- Project [project_id#1183, name#1411, desc#1422, goal#1480, keywords#1433, country#1492, CASE WHEN isnull(currency#1190) THEN   ELSE currency#1190 END AS currency#1504, final_status#1320, days_campaign#1456, hours_prepa#1468, text#1444]",
      "   +- Project [project_id#1183, name#1411, desc#1422, goal#1480, keywords#1433, CASE WHEN isnull(country#1335) THEN   ELSE country#1335 END AS country#1492, currency#1190, final_status#1320, days_campaign#1456, hours_prepa#1468, text#1444]",
      "      +- Project [project_id#1183, name#1411, desc#1422, CASE WHEN isnull(goal#1230) THEN -1 ELSE goal#1230 END AS goal#1480, keywords#1433, country#1335, currency#1190, final_status#1320, days_campaign#1456, hours_prepa#1468, text#1444]",
      "         +- Project [project_id#1183, name#1411, desc#1422, goal#1230, keywords#1433, country#1335, currency#1190, final_status#1320, days_campaign#1456, CASE WHEN isnull(hours_prepa#1387) THEN -1 ELSE hours_prepa#1387 END AS hours_prepa#1468, text#1444]",
      "            +- Project [project_id#1183, name#1411, desc#1422, goal#1230, keywords#1433, country#1335, currency#1190, final_status#1320, CASE WHEN isnull(days_campaign#1374) THEN -1 ELSE days_campaign#1374 END AS days_campaign#1456, hours_prepa#1387, text#1444]",
      "               +- Project [project_id#1183, name#1411, desc#1422, goal#1230, keywords#1433, country#1335, currency#1190, final_status#1320, days_campaign#1374, hours_prepa#1387, concat(name#1411,  , desc#1422,  , keywords#1433) AS text#1444]",
      "                  +- Project [project_id#1183, name#1411, desc#1422, goal#1230, lower(keywords#1187) AS keywords#1433, country#1335, currency#1190, final_status#1320, days_campaign#1374, hours_prepa#1387]",
      "                     +- Project [project_id#1183, name#1411, lower(desc#1185) AS desc#1422, goal#1230, keywords#1187, country#1335, currency#1190, final_status#1320, days_campaign#1374, hours_prepa#1387]",
      "                        +- Project [project_id#1183, lower(name#1184) AS name#1411, desc#1185, goal#1230, keywords#1187, country#1335, currency#1190, final_status#1320, days_campaign#1374, hours_prepa#1387]",
      "                           +- Project [project_id#1183, name#1184, desc#1185, goal#1230, keywords#1187, country#1335, currency#1190, final_status#1320, days_campaign#1374, hours_prepa#1387]",
      "                              +- Project [project_id#1183, name#1184, desc#1185, goal#1230, keywords#1187, country#1335, currency#1190, deadline#1245, created_at#1275, launched_at#1290, final_status#1320, days_campaign#1374, cast((cast((launched_at#1290 - created_at#1275) as double) / cast(60 as double)) as int) AS hours_prepa#1387]",
      "                                 +- Project [project_id#1183, name#1184, desc#1185, goal#1230, keywords#1187, country#1335, currency#1190, deadline#1245, created_at#1275, launched_at#1290, final_status#1320, datediff(cast(from_unixtime(cast(deadline#1245 as bigint), yyyy-MM-dd HH:mm:ss, Some(Europe/Paris)) as date), cast(from_unixtime(cast(launched_at#1290 as bigint), yyyy-MM-dd HH:mm:ss, Some(Europe/Paris)) as date)) AS days_campaign#1374]",
      "                                    +- Project [project_id#1183, name#1184, desc#1185, goal#1230, keywords#1187, country#1335, currency#1190, deadline#1245, created_at#1275, launched_at#1290, final_status#1320]",
      "                                       +- Filter currency#1190 RLIKE .{3}",
      "                                          +- Filter country#1335 RLIKE .{2}",
      "                                             +- Project [project_id#1183, name#1184, desc#1185, goal#1230, keywords#1187, country#1335, currency#1190, deadline#1245, state_changed_at#1260, created_at#1275, launched_at#1290, backers_count#1305, final_status#1320]",
      "                                                +- Filter ((disable_communication#1188 = True) || (disable_communication#1188 = False))",
      "                                                   +- Project [project_id#1183, name#1184, desc#1185, goal#1230, keywords#1187, disable_communication#1188, CASE WHEN (country#1189 = False) THEN currency#1190 ELSE country#1189 END AS country#1335, currency#1190, deadline#1245, state_changed_at#1260, created_at#1275, launched_at#1290, backers_count#1305, final_status#1320]",
      "                                                      +- Filter NOT isnull(state_changed_at#1260)",
      "                                                         +- Deduplicate [deadline#1245]",
      "                                                            +- Project [project_id#1183, name#1184, desc#1185, goal#1230, keywords#1187, disable_communication#1188, country#1189, currency#1190, deadline#1245, state_changed_at#1260, created_at#1275, launched_at#1290, backers_count#1305, cast(final_status#1196 as int) AS final_status#1320]",
      "                                                               +- Project [project_id#1183, name#1184, desc#1185, goal#1230, keywords#1187, disable_communication#1188, country#1189, currency#1190, deadline#1245, state_changed_at#1260, created_at#1275, launched_at#1290, cast(backers_count#1195 as int) AS backers_count#1305, final_status#1196]",
      "                                                                  +- Project [project_id#1183, name#1184, desc#1185, goal#1230, keywords#1187, disable_communication#1188, country#1189, currency#1190, deadline#1245, state_changed_at#1260, created_at#1275, cast(launched_at#1194 as int) AS launched_at#1290, backers_count#1195, final_status#1196]",
      "                                                                     +- Project [project_id#1183, name#1184, desc#1185, goal#1230, keywords#1187, disable_communication#1188, country#1189, currency#1190, deadline#1245, state_changed_at#1260, cast(created_at#1193 as int) AS created_at#1275, launched_at#1194, backers_count#1195, final_status#1196]",
      "                                                                        +- Project [project_id#1183, name#1184, desc#1185, goal#1230, keywords#1187, disable_communication#1188, country#1189, currency#1190, deadline#1245, cast(state_changed_at#1192 as int) AS state_changed_at#1260, created_at#1193, launched_at#1194, backers_count#1195, final_status#1196]",
      "                                                                           +- Project [project_id#1183, name#1184, desc#1185, goal#1230, keywords#1187, disable_communication#1188, country#1189, currency#1190, cast(deadline#1191 as int) AS deadline#1245, state_changed_at#1192, created_at#1193, launched_at#1194, backers_count#1195, final_status#1196]",
      "                                                                              +- Project [project_id#1183, name#1184, desc#1185, cast(goal#1186 as int) AS goal#1230, keywords#1187, disable_communication#1188, country#1189, currency#1190, deadline#1191, state_changed_at#1192, created_at#1193, launched_at#1194, backers_count#1195, final_status#1196]",
      "                                                                                 +- Relation[project_id#1183,name#1184,desc#1185,goal#1186,keywords#1187,disable_communication#1188,country#1189,currency#1190,deadline#1191,state_changed_at#1192,created_at#1193,launched_at#1194,backers_count#1195,final_status#1196] csv",
      "",
      "  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)",
      "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)",
      "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)",
      "  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)",
      "  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:104)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)",
      "  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)",
      "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)",
      "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)",
      "  at org.apache.spark.sql.Dataset.select(Dataset.scala:1340)",
      "  at org.apache.spark.sql.Dataset.select(Dataset.scala:1358)",
      "  ... 36 elided",
      ""
     ]
    }
   ],
   "source": [
    "dfCasted.select(\"goal\", \"backers_count\", \"final_status\").count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+-------------+------------+\n",
      "|                name|goal|backers_count|final_status|\n",
      "+--------------------+----+-------------+------------+\n",
      "| drawing for dollars|  20|            3|           1|\n",
      "|Sponsor Dereck Bl...| 300|            2|           0|\n",
      "|       Mr. Squiggles|  30|            0|           0|\n",
      "|Help me write my ...| 500|           18|           1|\n",
      "|Support casting m...|2000|            1|           0|\n",
      "+--------------------+----+-------------+------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------+--------------------+---------------------+--------+\n",
      "|country|            keywords|disable_communication|currency|\n",
      "+-------+--------------------+---------------------+--------+\n",
      "|     US| drawing-for-dollars|                False|     USD|\n",
      "|     US|sponsor-dereck-bl...|                False|     USD|\n",
      "|     US|        mr-squiggles|                False|     USD|\n",
      "|     US|help-me-write-my-...|                False|     USD|\n",
      "|     US|support-casting-m...|                False|     USD|\n",
      "+-------+--------------------+---------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+----------------+----------+-----------+\n",
      "|  deadline|state_changed_at|created_at|launched_at|\n",
      "+----------+----------------+----------+-----------+\n",
      "|1241333999|      1241334017|1240600507| 1240602723|\n",
      "|1242429000|      1242432018|1240960224| 1240975592|\n",
      "|1243027560|      1243027818|1242163613| 1242164398|\n",
      "|1243555740|      1243556121|1240963795| 1240966730|\n",
      "|1243769880|      1243770317|1241177914| 1241180541|\n",
      "+----------+----------------+----------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "n: Int = 5\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val n = 5\n",
    "\n",
    "dfCasted\n",
    "    .select(\"name\",\"goal\", \"backers_count\", \"final_status\")\n",
    "    .show(n)\n",
    "\n",
    "dfCasted\n",
    "    .select(\"country\",\"keywords\",\"disable_communication\",\"currency\")\n",
    "    .show(n)\n",
    "\n",
    "dfCasted\n",
    "    .select(\"deadline\",\"state_changed_at\",\"created_at\",\"launched_at\")\n",
    "    .show(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// dfCasted.groupBy(\"disable_communication\").count.orderBy($\"count\".desc).show(100)\n",
    "// dfCasted.groupBy(\"disable_communication\").count.orderBy($\"count\".desc).show\n",
    "// dfCasted.groupBy(\"country\").count.orderBy($\"count\".desc).show(100)\n",
    "// dfClean.groupBy(\"currency\").count.orderBy($\"count\".desc).show(100)\n",
    "// dfCasted.select(\"deadline\").dropDuplicates.show()\n",
    "\n",
    "// (dfCasted.select(\"deadline\").dropDuplicates.count()\n",
    "//  ,dfCasted.select(\"deadline\").count())\n",
    "\n",
    "// dfCasted.groupBy(\"state_changed_at\").count.orderBy($\"count\".desc).show(100)\n",
    "// dfCasted.groupBy(\"backers_count\").count.orderBy($\"count\".desc).show(100)\n",
    "// dfCasted.select(\"goal\", \"final_status\").show(30)\n",
    "// dfCasted.groupBy(\"country\", \"currency\").count.orderBy($\"count\".desc).show(50)\n",
    "\n",
    "// Cleaning à faire: \n",
    "// Only keep rows with \"True\" or \"False\" in disable_communication\n",
    "// drop disable_communication\n",
    "// drop rows where regex or different from the main countries (after\n",
    "// trying to fill with currency stage later)\n",
    "\n",
    "// same than above with the country culumn\n",
    "// dropduplicates in id column --done\n",
    "// filter rows where state_changed_at null\n",
    "// infer country from / currency if country is null (befre dropping countries)\n",
    "// US -> US , GB ->GB, CA->CA, AU-AU, NL->NL\n",
    "// "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfClean: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 9 more fields]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dfClean:DataFrame = dfCasted\n",
    "//     .dropDuplicates(\"deadline\")\n",
    "    .filter(!isnull($\"state_changed_at\"))\n",
    "    .withColumn(\"country\",when($\"country\" === \"False\",$\"currency\").otherwise($\"country\"))\n",
    "    .filter(($\"disable_communication\"===\"True\") || ($\"disable_communication\"===\"False\"))\n",
    "    .drop(\"disable_communication\")\n",
    "    .filter($\"country\" rlike \".{2}\")\n",
    "    .filter($\"currency\" rlike \".{3}\")\n",
    "    .drop(\"backers_count\",\"state_changed_at\")\n",
    "\n",
    "// dfClean\n",
    "//     .select(\"name\",\"goal\",\"final_status\")\n",
    "//     .show(n)\n",
    "\n",
    "// dfClean\n",
    "//     .select(\"country\",\"keywords\",\"currency\")\n",
    "//     .show(n)\n",
    "\n",
    "// dfClean\n",
    "//     .select(\"deadline\",\"created_at\",\"launched_at\")\n",
    "//     .show(n)\n",
    "\n",
    "// df.filter($\"country\" === \"False\")\n",
    "//   .groupBy(\"currency\")\n",
    "//   .count\n",
    "//   .orderBy($\"count\".desc)\n",
    "//   .show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Important:  col equivalent to $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Solution proposée\n",
    "\n",
    "// def cleanCountry(country: String, currency: String): String = {\n",
    "//   if (country == \"False\")\n",
    "//     currency\n",
    "//   else\n",
    "//     country\n",
    "// }\n",
    "\n",
    "// def cleanCurrency(currency: String): String = {\n",
    "//   if (currency != null && currency.length != 3)\n",
    "//     null\n",
    "//   else\n",
    "//     currency\n",
    "// }\n",
    "\n",
    "// val cleanCountryUdf = udf(cleanCountry _)\n",
    "// val cleanCurrencyUdf = udf(cleanCurrency _)\n",
    "\n",
    "// val dfCountry: DataFrame = dfNoFutur\n",
    "//   .withColumn(\"country2\", cleanCountryUdf($\"country\", $\"currency\"))\n",
    "//   .withColumn(\"currency2\", cleanCurrencyUdf($\"currency\"))\n",
    "//   .drop(\"country\", \"currency\")\n",
    "\n",
    "// // ou encore, en utilisant sql.functions.when:\n",
    "// dfNoFutur\n",
    "//   .withColumn(\"country2\", when($\"country\" === \"False\", $\"currency\").otherwise($\"country\"))\n",
    "//   .withColumn(\"currency2\", when($\"country\".isNotNull && length($\"currency\") =!= 3, null).otherwise($\"currency\"))\n",
    "//   .drop(\"country\", \"currency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.spark.sql.AnalysisException",
     "evalue": " cannot resolve '`deadline`' given input columns: [goal, currency, final_status, days_campaign, hours_prepa, desc, keywords, project_id, country, name, text];;",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.AnalysisException: cannot resolve '`deadline`' given input columns: [goal, currency, final_status, days_campaign, hours_prepa, desc, keywords, project_id, country, name, text];;",
      "'Project [project_id#10, name#238, desc#249, goal#307, keywords#260, country#343, currency#331, final_status#147, datediff(from_unixtime('deadline, yyyy-MM-dd HH:mm:ss, Some(Europe/Paris)), from_unixtime('launched_at, yyyy-MM-dd HH:mm:ss, Some(Europe/Paris))) AS days_campaign#378, hours_prepa#295, text#271]",
      "+- Filter currency#331 RLIKE .{3}",
      "   +- Filter country#343 RLIKE .{2}",
      "      +- Project [project_id#10, name#238, desc#249, goal#307, keywords#260, country#343, currency#331, final_status#147, days_campaign#283, hours_prepa#295, text#271]",
      "         +- Filter ((disable_communication#15 = True) || (disable_communication#15 = False))",
      "            +- Project [project_id#10, name#238, desc#249, goal#307, keywords#260, CASE WHEN (country#319 = False) THEN currency#331 ELSE country#319 END AS country#343, currency#331, final_status#147, days_campaign#283, hours_prepa#295, text#271, disable_communication#15]",
      "               +- Project [project_id#10, name#238, desc#249, goal#307, keywords#260, country#319, currency#331, final_status#147, days_campaign#283, hours_prepa#295, text#271, disable_communication#15]",
      "                  +- Filter NOT isnull(state_changed_at#87)",
      "                     +- Project [project_id#10, name#238, desc#249, goal#307, keywords#260, country#319, CASE WHEN isnull(currency#17) THEN   ELSE currency#17 END AS currency#331, final_status#147, days_campaign#283, hours_prepa#295, text#271, state_changed_at#87, disable_communication#15]",
      "                        +- Project [project_id#10, name#238, desc#249, goal#307, keywords#260, CASE WHEN isnull(country#162) THEN   ELSE country#162 END AS country#319, currency#17, final_status#147, days_campaign#283, hours_prepa#295, text#271, state_changed_at#87, disable_communication#15]",
      "                           +- Project [project_id#10, name#238, desc#249, CASE WHEN isnull(goal#57) THEN -1 ELSE goal#57 END AS goal#307, keywords#260, country#162, currency#17, final_status#147, days_campaign#283, hours_prepa#295, text#271, state_changed_at#87, disable_communication#15]",
      "                              +- Project [project_id#10, name#238, desc#249, goal#57, keywords#260, country#162, currency#17, final_status#147, days_campaign#283, CASE WHEN isnull(hours_prepa#214) THEN -1 ELSE hours_prepa#214 END AS hours_prepa#295, text#271, state_changed_at#87, disable_communication#15]",
      "                                 +- Project [project_id#10, name#238, desc#249, goal#57, keywords#260, country#162, currency#17, final_status#147, CASE WHEN isnull(days_campaign#201) THEN -1 ELSE days_campaign#201 END AS days_campaign#283, hours_prepa#214, text#271, state_changed_at#87, disable_communication#15]",
      "                                    +- Project [project_id#10, name#238, desc#249, goal#57, keywords#260, country#162, currency#17, final_status#147, days_campaign#201, hours_prepa#214, concat(name#238,  , desc#249,  , keywords#260) AS text#271, state_changed_at#87, disable_communication#15]",
      "                                       +- Project [project_id#10, name#238, desc#249, goal#57, lower(keywords#14) AS keywords#260, country#162, currency#17, final_status#147, days_campaign#201, hours_prepa#214, state_changed_at#87, disable_communication#15]",
      "                                          +- Project [project_id#10, name#238, lower(desc#12) AS desc#249, goal#57, keywords#14, country#162, currency#17, final_status#147, days_campaign#201, hours_prepa#214, state_changed_at#87, disable_communication#15]",
      "                                             +- Project [project_id#10, lower(name#11) AS name#238, desc#12, goal#57, keywords#14, country#162, currency#17, final_status#147, days_campaign#201, hours_prepa#214, state_changed_at#87, disable_communication#15]",
      "                                                +- Project [project_id#10, name#11, desc#12, goal#57, keywords#14, country#162, currency#17, final_status#147, days_campaign#201, hours_prepa#214, state_changed_at#87, disable_communication#15]",
      "                                                   +- Project [project_id#10, name#11, desc#12, goal#57, keywords#14, country#162, currency#17, deadline#72, created_at#102, launched_at#117, final_status#147, days_campaign#201, cast((cast((launched_at#117 - created_at#102) as double) / cast(60 as double)) as int) AS hours_prepa#214, state_changed_at#87, disable_communication#15]",
      "                                                      +- Project [project_id#10, name#11, desc#12, goal#57, keywords#14, country#162, currency#17, deadline#72, created_at#102, launched_at#117, final_status#147, datediff(cast(from_unixtime(cast(deadline#72 as bigint), yyyy-MM-dd HH:mm:ss, Some(Europe/Paris)) as date), cast(from_unixtime(cast(launched_at#117 as bigint), yyyy-MM-dd HH:mm:ss, Some(Europe/Paris)) as date)) AS days_campaign#201, state_changed_at#87, disable_communication#15]",
      "                                                         +- Project [project_id#10, name#11, desc#12, goal#57, keywords#14, country#162, currency#17, deadline#72, created_at#102, launched_at#117, final_status#147, state_changed_at#87, disable_communication#15]",
      "                                                            +- Filter currency#17 RLIKE .{3}",
      "                                                               +- Filter country#162 RLIKE .{2}",
      "                                                                  +- Project [project_id#10, name#11, desc#12, goal#57, keywords#14, country#162, currency#17, deadline#72, state_changed_at#87, created_at#102, launched_at#117, backers_count#132, final_status#147, disable_communication#15]",
      "                                                                     +- Filter ((disable_communication#15 = True) || (disable_communication#15 = False))",
      "                                                                        +- Project [project_id#10, name#11, desc#12, goal#57, keywords#14, disable_communication#15, CASE WHEN (country#16 = False) THEN currency#17 ELSE country#16 END AS country#162, currency#17, deadline#72, state_changed_at#87, created_at#102, launched_at#117, backers_count#132, final_status#147]",
      "                                                                           +- Filter NOT isnull(state_changed_at#87)",
      "                                                                              +- Deduplicate [deadline#72]",
      "                                                                                 +- Project [project_id#10, name#11, desc#12, goal#57, keywords#14, disable_communication#15, country#16, currency#17, deadline#72, state_changed_at#87, created_at#102, launched_at#117, backers_count#132, cast(final_status#23 as int) AS final_status#147]",
      "                                                                                    +- Project [project_id#10, name#11, desc#12, goal#57, keywords#14, disable_communication#15, country#16, currency#17, deadline#72, state_changed_at#87, created_at#102, launched_at#117, cast(backers_count#22 as int) AS backers_count#132, final_status#23]",
      "                                                                                       +- Project [project_id#10, name#11, desc#12, goal#57, keywords#14, disable_communication#15, country#16, currency#17, deadline#72, state_changed_at#87, created_at#102, cast(launched_at#21 as int) AS launched_at#117, backers_count#22, final_status#23]",
      "                                                                                          +- Project [project_id#10, name#11, desc#12, goal#57, keywords#14, disable_communication#15, country#16, currency#17, deadline#72, state_changed_at#87, cast(created_at#20 as int) AS created_at#102, launched_at#21, backers_count#22, final_status#23]",
      "                                                                                             +- Project [project_id#10, name#11, desc#12, goal#57, keywords#14, disable_communication#15, country#16, currency#17, deadline#72, cast(state_changed_at#19 as int) AS state_changed_at#87, created_at#20, launched_at#21, backers_count#22, final_status#23]",
      "                                                                                                +- Project [project_id#10, name#11, desc#12, goal#57, keywords#14, disable_communication#15, country#16, currency#17, cast(deadline#18 as int) AS deadline#72, state_changed_at#19, created_at#20, launched_at#21, backers_count#22, final_status#23]",
      "                                                                                                   +- Project [project_id#10, name#11, desc#12, cast(goal#13 as int) AS goal#57, keywords#14, disable_communication#15, country#16, currency#17, deadline#18, state_changed_at#19, created_at#20, launched_at#21, backers_count#22, final_status#23]",
      "                                                                                                      +- Relation[project_id#10,name#11,desc#12,goal#13,keywords#14,disable_communication#15,country#16,currency#17,deadline#18,state_changed_at#19,created_at#20,launched_at#21,backers_count#22,final_status#23] csv",
      "",
      "  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:111)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$3.applyOrElse(CheckAnalysis.scala:108)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:281)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:280)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:329)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:278)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:329)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:278)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:278)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:329)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:278)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$transformExpressionsUp$1.apply(QueryPlan.scala:93)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:105)",
      "  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpression$1(QueryPlan.scala:104)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:116)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1$2.apply(QueryPlan.scala:121)",
      "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)",
      "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)",
      "  at scala.collection.immutable.List.foreach(List.scala:392)",
      "  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)",
      "  at scala.collection.immutable.List.map(List.scala:296)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$1(QueryPlan.scala:121)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:126)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.mapExpressions(QueryPlan.scala:126)",
      "  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:93)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:108)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:86)",
      "  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:127)",
      "  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:86)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:95)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:108)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)",
      "  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)",
      "  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)",
      "  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)",
      "  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)",
      "  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)",
      "  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withPlan(Dataset.scala:3412)",
      "  at org.apache.spark.sql.Dataset.select(Dataset.scala:1340)",
      "  at org.apache.spark.sql.Dataset.withColumns(Dataset.scala:2258)",
      "  at org.apache.spark.sql.Dataset.withColumn(Dataset.scala:2225)",
      "  ... 37 elided",
      ""
     ]
    }
   ],
   "source": [
    "val df2:DataFrame = dfClean\n",
    "    .withColumn(\"days_campaign\",datediff(from_unixtime($\"deadline\"),from_unixtime($\"launched_at\")))\n",
    "    .withColumn(\"hours_prepa\",(($\"launched_at\"-$\"created_at\")/60).cast(\"Int\"))\n",
    "    .drop(\"launched_at\",\"deadline\",\"created_at\")\n",
    "    .withColumn(\"name\",lower($\"name\"))\n",
    "    .withColumn(\"desc\",lower($\"desc\"))\n",
    "    .withColumn(\"keywords\",lower($\"keywords\"))\n",
    "    \n",
    "\n",
    "val df3:DataFrame = df2\n",
    "    .withColumn(\"text\",concat($\"name\",lit(\" \"),$\"desc\",lit(\" \"),$\"keywords\"))\n",
    "    .withColumn(\"days_campaign\",when(isnull($\"days_campaign\"),-1).otherwise($\"days_campaign\"))\n",
    "    .withColumn(\"hours_prepa\",when(isnull($\"hours_prepa\"),-1).otherwise($\"hours_prepa\"))\n",
    "    .withColumn(\"goal\",when(isnull($\"goal\"),-1).otherwise($\"goal\"))\n",
    "    .withColumn(\"country\",when(isnull($\"country\"),\" \").otherwise($\"country\"))\n",
    "    .withColumn(\"currency\",when(isnull($\"currency\"),\" \").otherwise($\"currency\"))\n",
    "\n",
    "// df3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "36: error: not found: value df3",
     "output_type": "error",
     "traceback": [
      "<console>:36: error: not found: value df3",
      "       df3",
      "       ^",
      "<console>:38: error: not found: value n",
      "           .show(n)",
      "                 ^",
      "<console>:26: error: not found: value df3",
      "       df3.select(\"name\",\"goal\",\"final_status\")",
      "       ^",
      "<console>:27: error: not found: value n",
      "           .show(n)",
      "                 ^",
      "<console>:29: error: not found: value df3",
      "       df3",
      "       ^",
      "<console>:31: error: not found: value n",
      "           .show(n)",
      "                 ^",
      ""
     ]
    }
   ],
   "source": [
    "df3\n",
    "    .select(\"name\",\"goal\",\"final_status\")\n",
    "    .show(n)\n",
    "\n",
    "df3\n",
    "    .select(\"country\",\"keywords\",\"currency\")\n",
    "    .show(n)\n",
    "\n",
    "\n",
    "df3\n",
    "    .select(\"days_campaign\",\"hours_prepa\",\"text\")\n",
    "    .show(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.write.parquet(\"/home/jorge/Documents/Git/spark_project_kickstarter_2019_2020/cleanData.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "28: error: not found: value df3",
     "output_type": "error",
     "traceback": [
      "<console>:28: error: not found: value df3",
      "       df3.show(5)",
      "       ^",
      ""
     ]
    }
   ],
   "source": [
    "df3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy for Model 1 = 0.6875604716253096\n",
      "numFeatures = 35.0\n",
      "regParam = 1.0E-9\n",
      "Train dataset size is : 90932\n",
      "Test dataset size is : 9992\n",
      "Test set accuracy for Model 1 = 0.6875604716253096\n",
      "Test set accuracy for the best model of the Grid Search is = 0.7303824882872976\n",
      "regParam = 1.0E-9\n",
      "minDF = 35.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.SparkConf\n",
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
       "import org.apache.spark.sql.functions._\n",
       "import org.apache.spark.SparkConf\n",
       "import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel, IDF, OneHotEncoderEstimator, RegexTokenizer, StopWordsRemover, StringIndexer, VectorAssembler}\n",
       "import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel}\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.ml.tuning.{CrossValidator, CrossValidatorModel, ParamGridBuilder, TrainValidationSplit}\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "import org.apache.spark.ml.param.ParamMap\n",
       "conf: org.apache.spark.SparkConf = org.apache.spark.Spark..."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.ml.feature.{CountVectorizer, CountVectorizerModel, IDF, OneHotEncoderEstimator, RegexTokenizer, StopWordsRemover, StringIndexer, VectorAssembler}\n",
    "import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, CrossValidatorModel, ParamGridBuilder, TrainValidationSplit}\n",
    "// import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
    "// import org.apache.spark.ml.evaluation\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "\n",
    "\n",
    "val conf = new SparkConf().setAll(Map(\n",
    "  \"spark.scheduler.mode\" -> \"FIFO\",\n",
    "  \"spark.speculation\" -> \"false\",\n",
    "  \"spark.reducer.maxSizeInFlight\" -> \"48m\",\n",
    "  \"spark.serializer\" -> \"org.apache.spark.serializer.KryoSerializer\",\n",
    "  \"spark.kryoserializer.buffer.max\" -> \"1g\",\n",
    "  \"spark.shuffle.file.buffer\" -> \"32k\",\n",
    "  \"spark.default.parallelism\" -> \"12\",\n",
    "  \"spark.sql.shuffle.partitions\" -> \"12\",\n",
    "  \"spark.driver.maxResultSize\" -> \"2g\"\n",
    "))\n",
    "\n",
    "val spark = SparkSession\n",
    "  .builder\n",
    "  .config(conf)\n",
    "  .appName(\"TP Spark : Trainer\")\n",
    "  .getOrCreate()\n",
    "\n",
    "\n",
    "/*******************************************************************************\n",
    "  *\n",
    "  *       TP 3\n",
    "  *\n",
    "  *       - lire le fichier sauvegarder précédemment\n",
    "  *       - construire les Stages du pipeline, puis les assembler\n",
    "  *       - trouver les meilleurs hyperparamètres pour l'entraînement du pipeline avec une grid-search\n",
    "  *       - Sauvegarder le pipeline entraîné\n",
    "  *\n",
    "  *       if problems with unimported modules => sbt plugins update\n",
    "  *\n",
    "  ********************************************************************************/\n",
    "// val df:DataFrame = spark.read.parquet(\"prepared_trainingset/\")\n",
    "val df:DataFrame = spark.read.parquet(\"cleanData.parquet/\")\n",
    "    .filter(!isnull($\"text\"))\n",
    "    .filter(!($\"country\" rlike \"DE\"))\n",
    "\n",
    "\n",
    "\n",
    "val tokenizer = new RegexTokenizer()\n",
    "  .setPattern(\"\\\\W+\")\n",
    "  .setGaps(true)\n",
    "  .setInputCol(\"text\")\n",
    "  .setOutputCol(\"tokens\")\n",
    "\n",
    "val remover = new StopWordsRemover()\n",
    "  .setInputCol(tokenizer.getOutputCol)\n",
    "  .setOutputCol(\"filtered\")\n",
    "\n",
    "val cvModel: CountVectorizer = new CountVectorizer()\n",
    "  .setInputCol(remover.getOutputCol)\n",
    "  .setOutputCol(\"vect\")\n",
    "  .setMinDF(50)\n",
    "\n",
    "val idf = new IDF()\n",
    "  .setInputCol(cvModel.getOutputCol)\n",
    "  .setOutputCol(\"tfidf\")\n",
    "\n",
    "val indexerCountry = new StringIndexer()\n",
    "  .setInputCol(\"country\")\n",
    "  .setOutputCol(\"country_indexed\")\n",
    "\n",
    "val indexerCurrency = new StringIndexer()\n",
    "  .setInputCol(\"currency\")\n",
    "  .setOutputCol(\"currency_indexed\")\n",
    "\n",
    "val encoder = new OneHotEncoderEstimator()\n",
    "  .setInputCols(Array(\"country_indexed\", \"currency_indexed\"))\n",
    "  .setOutputCols(Array(\"country_onehot\", \"currency_onehot\"))\n",
    "\n",
    "val assembler = new VectorAssembler()\n",
    "  .setInputCols(Array(\"tfidf\",\"days_campaign\",\"hours_prepa\",\"goal\",\"country_onehot\",\"currency_onehot\"))\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "val lr = new LogisticRegression()\n",
    "  .setElasticNetParam(0.0)\n",
    "  .setFitIntercept(true)\n",
    "  .setFeaturesCol(\"features\")\n",
    "  .setLabelCol(\"final_status\")\n",
    "  .setStandardization(true)\n",
    "  .setPredictionCol(\"predictions\")\n",
    "  .setRawPredictionCol(\"raw_predictions\")\n",
    "//   .setThresholds(Array(0.7, 0.3))\n",
    "//   .setTol(1.0e-6)\n",
    "//   .setMaxIter(20)\n",
    "\n",
    "val pipeline = new Pipeline()\n",
    "  .setStages(Array(tokenizer,remover,cvModel,idf,indexerCountry, indexerCurrency,encoder, assembler,lr))\n",
    "\n",
    "\n",
    "val Array(train,test) = df.randomSplit(Array[Double](0.9, 0.1))\n",
    "val size = (train.count,test.count)\n",
    "\n",
    "\n",
    "val model1 = pipeline.fit(train)\n",
    "val predictions = model1.transform(test)\n",
    "\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"final_status\")\n",
    "  .setPredictionCol(\"predictions\")\n",
    "  .setMetricName(\"f1\")\n",
    "\n",
    "val f1 = evaluator.evaluate(predictions)\n",
    "println(\"Test set accuracy for Model 1 = \" + f1)\n",
    "\n",
    "\n",
    "val grid = new ParamGridBuilder()\n",
    "  .addGrid(lr.regParam,Array(10e-10,10e-8,10e-6,10e-4,10e-2))\n",
    "  .addGrid(cvModel.minDF,Array(20.0,35.0,55.0,75.0,95.0))\n",
    "  .build()\n",
    "\n",
    "val trainValidationSplit = new TrainValidationSplit()\n",
    "  .setEstimator(pipeline)\n",
    "  .setEvaluator(evaluator)\n",
    "  .setEstimatorParamMaps(grid)\n",
    "  // 80% of the data will be used for training and the remaining 20% for validation.\n",
    "  .setTrainRatio(0.8)\n",
    "\n",
    "val gridSearch = trainValidationSplit.fit(df)\n",
    "val gridSearchBestModel = gridSearch.bestModel\n",
    "\n",
    "val f1best = evaluator.evaluate(gridSearchBestModel.transform(test))\n",
    "\n",
    "\n",
    "val bestPipelineModel = gridSearchBestModel.asInstanceOf[PipelineModel]\n",
    "val stages = bestPipelineModel.stages\n",
    "val cvStage = stages(2).asInstanceOf[CountVectorizerModel]\n",
    "println(\"numFeatures = \" + cvStage.getMinDF)\n",
    "val lrStage = stages(8).asInstanceOf[LogisticRegressionModel]\n",
    "println(\"regParam = \" + lrStage.getRegParam)\n",
    "\n",
    "\n",
    "//    predictions.groupBy(\"final_status\", \"predictions\").count.show()\n",
    "println(\"Train dataset size is : \" + size._1)\n",
    "println(\"Test dataset size is : \" + size._2)\n",
    "println(\"Test set accuracy for Model 1 = \" + f1)\n",
    "println(\"Test set accuracy for the best model of the Grid Search is = \" + f1best)\n",
    "println(\"regParam = \" + lrStage.getRegParam)\n",
    "println(\"minDF = \" + cvStage.getMinDF)\n",
    "//    println(\"Params for best model are : \" + gridSearchBestModel.getParam(lr.getRegParam))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res21: String =\n",
       "collectSubModels: whether to collect a list of sub-models trained during tuning. If set to false, then only the single best sub-model will be available after fitting. If set to true, then all sub-models will be available. Warning: For large models, collecting all sub-models can cause OOMs on the Spark driver (default: false)\n",
       "estimator: estimator for selection (current: pipeline_1b6c3ce11f36)\n",
       "estimatorParamMaps: param maps for the estimator (current: [Lorg.apache.spark.ml.param.ParamMap;@5b06b566)\n",
       "evaluator: evaluator used to select hyper-parameters that maximize the validated metric (current: mcEval_030e736e766e)\n",
       "parallelism: the number of threads to use when running parallel algorithms (default: 1)\n",
       "seed: random seed (default: -1772833110)\n",
       "trainRatio: ratio between train..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridSearch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numFeatures = 35.0\n",
      "regParam = 1.0E-9\n",
      "Train dataset size is : 90930\n",
      "Test dataset size is : 9994\n",
      "Test set accuracy for Model 1 = 0.654367644345559\n",
      "Test set accuracy for the best model of the Grid Search is = 0.7046142667490585\n",
      "regParam = 1.0E-9\n",
      "minDF = 35.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "f1best: Double = 0.7046142667490585\n",
       "bestPipelineModel: org.apache.spark.ml.PipelineModel = pipeline_1b6c3ce11f36\n",
       "stages: Array[org.apache.spark.ml.Transformer] = Array(regexTok_f49dd8aaa75b, stopWords_ea9073cb3311, cntVec_57221211936f, idf_78484bd92bbf, strIdx_c3a4316e21e6, strIdx_4f78098f12d2, oneHotEncoder_9d2d1fa42661, vecAssembler_45d2dc3e69c7, LogisticRegressionModel: uid = logreg_5696f06e88aa, numClasses = 2, numFeatures = 5536)\n",
       "cvStage: org.apache.spark.ml.feature.CountVectorizerModel = cntVec_57221211936f\n",
       "lrStage: org.apache.spark.ml.classification.LogisticRegressionModel = LogisticRegressionModel: uid = logreg_5696f06e88aa, numClasses = 2, numFeatures = 5536\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val f1best = evaluator.evaluate(gridSearchBestModel.transform(test))\n",
    "val bestPipelineModel = gridSearchBestModel.asInstanceOf[PipelineModel]\n",
    "val stages = bestPipelineModel.stages\n",
    "val cvStage = stages(2).asInstanceOf[CountVectorizerModel]\n",
    "println(\"numFeatures = \" + cvStage.getMinDF)\n",
    "val lrStage = stages(8).asInstanceOf[LogisticRegressionModel]\n",
    "println(\"regParam = \" + lrStage.getRegParam)\n",
    "\n",
    "\n",
    "//    predictions.groupBy(\"final_status\", \"predictions\").count.show()\n",
    "println(\"Train dataset size is : \" + size._1)\n",
    "println(\"Test dataset size is : \" + size._2)\n",
    "println(\"Test set accuracy for Model 1 = \" + f1)\n",
    "println(\"Test set accuracy for the best model of the Grid Search is = \" + f1best)\n",
    "println(\"regParam = \" + lrStage.getRegParam)\n",
    "println(\"minDF = \" + cvStage.getMinDF)\n",
    "//    println(\"Params for best model are : \" + gridSearchBestModel.getParam(lr.getRegParam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|country|count|\n",
      "+-------+-----+\n",
      "|     DE|    1|\n",
      "+-------+-----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [project_id: string, name: string ... 9 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// val df:DataFrame = spark.read.parquet(\"prepared_trainingset/\")\n",
    "val df:DataFrame = spark.read.parquet(\"cleanData.parquet/\")\n",
    "    .filter(!isnull($\"text\"))\n",
    "    .filter($\"country\" rlike \"DE\")\n",
    "\n",
    "// predictions.select(\"predictions\").show\n",
    "df.select(\"country\").groupBy(\"country\").count.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|test| count|\n",
      "+----+------+\n",
      "|   0|100933|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"test\", when(isnan($\"goal\"),1).otherwise(0)).groupBy(\"test\").count.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// df.select(\"country\").map(line => (line.toString(),line.toString.length())).orderBy($\"_2\".desc).show(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
